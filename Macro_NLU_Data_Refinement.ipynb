{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nlu_engine import NLUEngine\n",
    "from nlu_engine import MacroDataRefinement\n",
    "from nlu_engine import MacroIntentRefinement\n",
    "\n",
    "from nlu_engine import DataUtils\n",
    "from nlu_engine import RenderJSON\n",
    "\n",
    "from nlu_engine import Analytics\n",
    "\n",
    "from nlu_engine import IntentMatcher, LR\n",
    "from nlu_engine import EntityExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, youâ€™ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "NOTE: We will use Logistic Regression with TFIDF features to train our intent models and CRFs for entity exraction. Why? Well, they are very fast and both methods aren't state-of-the-art. This is good, because it is easier to find problems we will need to refine in the dataset than if we were to use a proper NLU engine like Snips or something SOTA like BERT. It is very important to note that some of the the problems we will pick up on, might not be an actual issue, but might be due to the limitations of the models. Refining the real problems and ignoring the limitations of the models is a good way to improve the models. Then when the dataset is ready, we can use some more advanced NLU engine and get the best performance possible.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded nlu_data_refined_df.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlu_data_df = pd.read_csv(\n",
    "        'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)\n",
    "    print('Successfully loaded nlu_data_refined_df.csv')\n",
    "except:\n",
    "    data = 'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    "    nlu_data_df = DataUtils.load_data(\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this when done.It's just for testing!\n",
    "data = 'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    "nlu_data_df = DataUtils.load_data(\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upgrade our data set to include some new columns we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = DataUtils.upgrade_dataframe(nlu_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intent classifier report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a report by domain classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression(random_state=0, solver='liblinear')\n",
      "Cross validating with LogisticRegression(random_state=0, solver='liblinear')\n",
      "Time it took to cross validate LogisticRegression(random_state=0, solver='liblinear'): 5.212929725646973\n",
      "Generating report for LogisticRegression(random_state=0, solver='liblinear')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alarm</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.899174</td>\n",
       "      <td>0.942808</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calendar</td>\n",
       "      <td>0.841991</td>\n",
       "      <td>0.927768</td>\n",
       "      <td>0.882801</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cooking</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.675743</td>\n",
       "      <td>0.785612</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datetime</td>\n",
       "      <td>0.849254</td>\n",
       "      <td>0.816356</td>\n",
       "      <td>0.832480</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>email</td>\n",
       "      <td>0.955677</td>\n",
       "      <td>0.912993</td>\n",
       "      <td>0.933848</td>\n",
       "      <td>1724.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>general</td>\n",
       "      <td>0.815404</td>\n",
       "      <td>0.832813</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>6089.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iot</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.936728</td>\n",
       "      <td>0.953653</td>\n",
       "      <td>1296.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lists</td>\n",
       "      <td>0.924945</td>\n",
       "      <td>0.881178</td>\n",
       "      <td>0.902531</td>\n",
       "      <td>951.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>music</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.660617</td>\n",
       "      <td>0.763103</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news</td>\n",
       "      <td>0.869388</td>\n",
       "      <td>0.744755</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>play</td>\n",
       "      <td>0.889581</td>\n",
       "      <td>0.855831</td>\n",
       "      <td>0.872379</td>\n",
       "      <td>2504.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qa</td>\n",
       "      <td>0.660120</td>\n",
       "      <td>0.887248</td>\n",
       "      <td>0.757015</td>\n",
       "      <td>2235.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>recommendation</td>\n",
       "      <td>0.839442</td>\n",
       "      <td>0.705279</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>social</td>\n",
       "      <td>0.957411</td>\n",
       "      <td>0.793785</td>\n",
       "      <td>0.867954</td>\n",
       "      <td>708.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>takeaway</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.945571</td>\n",
       "      <td>0.893145</td>\n",
       "      <td>0.918611</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>weather</td>\n",
       "      <td>0.918511</td>\n",
       "      <td>0.885548</td>\n",
       "      <td>0.901728</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.897621</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.855921</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.863054</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.854831</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            domain  precision    recall  f1-score       support  \\\n",
       "0            alarm   0.990893  0.899174  0.942808    605.000000   \n",
       "1            audio   0.950000  0.756219  0.842105    402.000000   \n",
       "2         calendar   0.841991  0.927768  0.882801   2935.000000   \n",
       "3          cooking   0.938144  0.675743  0.785612    404.000000   \n",
       "4         datetime   0.849254  0.816356  0.832480    697.000000   \n",
       "5            email   0.955677  0.912993  0.933848   1724.000000   \n",
       "6          general   0.815404  0.832813  0.824017   6089.000000   \n",
       "7              iot   0.971200  0.936728  0.953653   1296.000000   \n",
       "8            lists   0.924945  0.881178  0.902531    951.000000   \n",
       "9            music   0.903226  0.660617  0.763103    551.000000   \n",
       "10            news   0.869388  0.744755  0.802260    858.000000   \n",
       "11            play   0.889581  0.855831  0.872379   2504.000000   \n",
       "12              qa   0.660120  0.887248  0.757015   2235.000000   \n",
       "13  recommendation   0.839442  0.705279  0.766534    682.000000   \n",
       "14          social   0.957411  0.793785  0.867954    708.000000   \n",
       "15        takeaway   0.936416  0.790244  0.857143    410.000000   \n",
       "16       transport   0.945571  0.893145  0.918611    992.000000   \n",
       "17         weather   0.918511  0.885548  0.901728   1031.000000   \n",
       "18        accuracy   0.853673  0.853673  0.853673      0.853673   \n",
       "19       macro avg   0.897621  0.825301  0.855921  25074.000000   \n",
       "20    weighted avg   0.863054  0.853673  0.854831  25074.000000   \n",
       "\n",
       "            classifier encoding  \n",
       "0   LogisticRegression    tfidf  \n",
       "1   LogisticRegression    tfidf  \n",
       "2   LogisticRegression    tfidf  \n",
       "3   LogisticRegression    tfidf  \n",
       "4   LogisticRegression    tfidf  \n",
       "5   LogisticRegression    tfidf  \n",
       "6   LogisticRegression    tfidf  \n",
       "7   LogisticRegression    tfidf  \n",
       "8   LogisticRegression    tfidf  \n",
       "9   LogisticRegression    tfidf  \n",
       "10  LogisticRegression    tfidf  \n",
       "11  LogisticRegression    tfidf  \n",
       "12  LogisticRegression    tfidf  \n",
       "13  LogisticRegression    tfidf  \n",
       "14  LogisticRegression    tfidf  \n",
       "15  LogisticRegression    tfidf  \n",
       "16  LogisticRegression    tfidf  \n",
       "17  LogisticRegression    tfidf  \n",
       "18  LogisticRegression    tfidf  \n",
       "19  LogisticRegression    tfidf  \n",
       "20  LogisticRegression    tfidf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_labels = 'scenario'\n",
    "\n",
    "domain_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict=domain_labels,\n",
    "    classifier=LR\n",
    ")\n",
    "\n",
    "domain_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be easier to see this graphed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(domain_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's do a report by intent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression(random_state=0, solver='liblinear')\n",
      "Cross validating with LogisticRegression(random_state=0, solver='liblinear')\n",
      "Time it took to cross validate LogisticRegression(random_state=0, solver='liblinear'): 19.36682653427124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for LogisticRegression(random_state=0, solver='liblinear')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affirm</td>\n",
       "      <td>0.988848</td>\n",
       "      <td>0.960289</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alarm_query</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alarm_set</td>\n",
       "      <td>0.778878</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.798646</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>wemo_off</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.850829</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.833812</td>\n",
       "      <td>0.833812</td>\n",
       "      <td>0.833812</td>\n",
       "      <td>0.833812</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.864908</td>\n",
       "      <td>0.737744</td>\n",
       "      <td>0.780757</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.856249</td>\n",
       "      <td>0.833812</td>\n",
       "      <td>0.836555</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          intent  precision    recall  f1-score       support  \\\n",
       "0     addcontact   0.909091  0.574713  0.704225     87.000000   \n",
       "1         affirm   0.988848  0.960289  0.974359    554.000000   \n",
       "2    alarm_query   0.906250  0.725000  0.805556    200.000000   \n",
       "3   alarm_remove   0.938462  0.521368  0.670330    117.000000   \n",
       "4      alarm_set   0.778878  0.819444  0.798646    288.000000   \n",
       "..           ...        ...       ...       ...           ...   \n",
       "65      wemo_off   0.927711  0.785714  0.850829     98.000000   \n",
       "66       wemo_on   0.981818  0.710526  0.824427     76.000000   \n",
       "67      accuracy   0.833812  0.833812  0.833812      0.833812   \n",
       "68     macro avg   0.864908  0.737744  0.780757  25074.000000   \n",
       "69  weighted avg   0.856249  0.833812  0.836555  25074.000000   \n",
       "\n",
       "            classifier encoding  \n",
       "0   LogisticRegression    tfidf  \n",
       "1   LogisticRegression    tfidf  \n",
       "2   LogisticRegression    tfidf  \n",
       "3   LogisticRegression    tfidf  \n",
       "4   LogisticRegression    tfidf  \n",
       "..                 ...      ...  \n",
       "65  LogisticRegression    tfidf  \n",
       "66  LogisticRegression    tfidf  \n",
       "67  LogisticRegression    tfidf  \n",
       "68  LogisticRegression    tfidf  \n",
       "69  LogisticRegression    tfidf  \n",
       "\n",
       "[70 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_labels= 'intent'\n",
    "\n",
    "intent_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict=intent_labels,\n",
    "    classifier=LR\n",
    ")\n",
    "intent_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph might also be nice here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(intent_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro Intent Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">For changing to another domain, start here again.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a classifier with the current state of the data and get the predicted intent labels.\n",
    "\n",
    "<span style=\"color: red;\">If you have already performed refinements, this will refresh the predicted labels.</span>\n",
    "\n",
    "(why not split a training test set? Because we want to see the results of the intent classifier on the whole data set, I mean if it's still getting it wrong when it has trained on it, then perhaps there is something wrong with the utterance, tagging, overlapping intents, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression(random_state=0, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "\n",
    "nlu_data_df = IntentMatcher.get_predicted_labels(\n",
    "    nlu_data_df, LR_intent_classifier_model, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what works and what doesn't, we can start refining the intents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each domain, this step will be repeated until all intents have been refined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a domain (scenario) to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(nlu_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get all of the entries for that domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataframe\n"
     ]
    }
   ],
   "source": [
    "domain_df = DataUtils.get_domain_df(nlu_data_df, domain_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the intent keyword features and their rankings (coefs) from the intent classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: This will be removed in a later version (no need to rank in the reports, it isn't so helpful!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_feature_rank_df = MacroIntentRefinement.intent_keyword_feature_rankings(\n",
    "    LR_intent_classifier_model, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having all of the incorrectly predicted intents to review for this domain is a good way to see what is going wrong. Especially when you have used them all as a training set, and yet it still can't predict some entries correctly.\n",
    "\n",
    "The big question is: Is it because of defects in the data or is it because of the intent classifier? We really want to find defects in the data to refine over classifier defects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>answer_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>stop seven am alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>stop [time : seven am] alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>delete the alarm i just set</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>delete the alarm i just set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>alert me at three pm to goto the concert</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>calendar_set</td>\n",
       "      <td>alert me at [time : three pm] to goto the [eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>How would you ask your PDA to tell you about t...</td>\n",
       "      <td>did i set an alarm to wake up in the morning</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>did i set an alarm to [alarm_type : wake up] i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>How would you ask your PDA to tell you about t...</td>\n",
       "      <td>do i have an alarm set</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>do i have an alarm set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>set a repeating reminder alarm for the faceboo...</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>calendar_set</td>\n",
       "      <td>set a repeating reminder alarm for the [media_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10270</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>set an alert for two pm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>calendar_set</td>\n",
       "      <td>set an alert for [time : two pm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10469</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>schedule an alarm for next weeks dentist appoi...</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>calendar_query</td>\n",
       "      <td>schedule an alarm for [date : next weeks] [eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10818</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>set alarm for this event reminder repeating fo...</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>calendar_set</td>\n",
       "      <td>set alarm for this event reminder [event_name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17347</th>\n",
       "      <td>How would you ask your PDA a general knowledge...</td>\n",
       "      <td>when was the last alarm set</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>when was the last alarm set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "44     Write what you would tell your PDA in the foll...   \n",
       "179    How would you ask your PDA to remove an alarm ...   \n",
       "210    Write what you would tell your PDA in the foll...   \n",
       "212    How would you ask your PDA to tell you about t...   \n",
       "512    How would you ask your PDA to tell you about t...   \n",
       "...                                                  ...   \n",
       "10258  Write what you would tell your PDA in the foll...   \n",
       "10270  Write what you would tell your PDA in the foll...   \n",
       "10469  Write what you would tell your PDA in the foll...   \n",
       "10818  Write what you would tell your PDA in the foll...   \n",
       "17347  How would you ask your PDA a general knowledge...   \n",
       "\n",
       "                                       answer_normalised scenario  \\\n",
       "44                                   stop seven am alarm    alarm   \n",
       "179                          delete the alarm i just set    alarm   \n",
       "210             alert me at three pm to goto the concert    alarm   \n",
       "212         did i set an alarm to wake up in the morning    alarm   \n",
       "512                               do i have an alarm set    alarm   \n",
       "...                                                  ...      ...   \n",
       "10258  set a repeating reminder alarm for the faceboo...    alarm   \n",
       "10270                            set an alert for two pm    alarm   \n",
       "10469  schedule an alarm for next weeks dentist appoi...    alarm   \n",
       "10818  set alarm for this event reminder repeating fo...    alarm   \n",
       "17347                        when was the last alarm set    alarm   \n",
       "\n",
       "             intent predicted_label  \\\n",
       "44     alarm_remove       alarm_set   \n",
       "179    alarm_remove       alarm_set   \n",
       "210       alarm_set    calendar_set   \n",
       "212     alarm_query       alarm_set   \n",
       "512     alarm_query       alarm_set   \n",
       "...             ...             ...   \n",
       "10258     alarm_set    calendar_set   \n",
       "10270     alarm_set    calendar_set   \n",
       "10469     alarm_set  calendar_query   \n",
       "10818     alarm_set    calendar_set   \n",
       "17347   alarm_query       alarm_set   \n",
       "\n",
       "                                       answer_annotation  \n",
       "44                          stop [time : seven am] alarm  \n",
       "179                          delete the alarm i just set  \n",
       "210    alert me at [time : three pm] to goto the [eve...  \n",
       "212    did i set an alarm to [alarm_type : wake up] i...  \n",
       "512                               do i have an alarm set  \n",
       "...                                                  ...  \n",
       "10258  set a repeating reminder alarm for the [media_...  \n",
       "10270                   set an alert for [time : two pm]  \n",
       "10469  schedule an alarm for [date : next weeks] [eve...  \n",
       "10818  set alarm for this event reminder [event_name ...  \n",
       "17347                        when was the last alarm set  \n",
       "\n",
       "[110 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_intent_predictions_df = IntentMatcher.get_incorrect_predicted_labels(\n",
    "    domain_df, LR_intent_classifier_model, tfidf_vectorizer)\n",
    "incorrect_intent_predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be a bit much seeing everything that isn't working right, perhaps we can break it down better in a report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the report. You can use the circle with the plus to expand the items individually in the report or click on the number of items. Might I recommend looking at one intent at a time and expanding the nested items for each of those. There is a lot of information to look at here, but this stuff is super important to understand for the refinement of the data.\n",
    "\n",
    "Each intent has the following items:\n",
    "* **f1 score**: the overall score of the intent(we wanrt to improve this number!)\n",
    "* **total count**: the total number of utterances that have this intent\n",
    "* **total incorrect count**: the total number of utterances that have this intent but are incorrectly predicted(we want to reduce this number!)\n",
    "* **top features**: the top ten features(words) that are associated with this intent(these are just the individual words ranked, not combined together!)\n",
    "* **overlapping features**: the top ten features(words) that are associated with this intent and are also associated with other intents that may make classification based solely on these features difficult\n",
    "* **correct utterance example**: the intent, the first annotated utterance that is correctly predicted as an example, and a list of the words in the utterance with their coefficient rankings\n",
    "* **incorrect utterance example**: the intent, the first annotated utterance that is incorrectly predicted as an example, and a list of the words in the utterance with their coefficient rankings\n",
    "* **incorrect predicted intents and counts**: for this intent, a list of the incorrectly predicted intents and their counts(we want to reduce this!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"8a1911a0-8344-48c3-b494-3ebe034559ed\" style=\"height: 600px; width:100%;\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n        document.getElementById('8a1911a0-8344-48c3-b494-3ebe034559ed').appendChild(renderjson({\"alarm_remove\": {\"f1_score\": 0.67, \"total_count\": 117, \"total_incorrect_count\": 35, \"top_features\": [\"alarm\", \"remove\", \"alarms\", \"cancel\", \"off\", \"removed\", \"disable\", \"delete\", \"wake\", \"rid\"], \"overlapping_features\": [{\"intent\": \"alarm_query\", \"feature\": \"alarms\", \"coef\": 10.161784835394235}, {\"intent\": \"alarm_set\", \"feature\": \"alarm\", \"coef\": 8.851573863665756}, {\"intent\": \"hue_lightoff\", \"feature\": \"off\", \"coef\": 8.829801974132103}, {\"intent\": \"calendar_remove\", \"feature\": \"delete\", \"coef\": 8.530851319752372}, {\"intent\": \"lists_remove\", \"feature\": \"remove\", \"coef\": 7.891740024626676}, {\"intent\": \"alarm_query\", \"feature\": \"alarm\", \"coef\": 7.431133772118585}, {\"intent\": \"calendar_remove\", \"feature\": \"remove\", \"coef\": 7.309990837754691}, {\"intent\": \"lists_remove\", \"feature\": \"delete\", \"coef\": 6.952396920133956}, {\"intent\": \"wemo_off\", \"feature\": \"off\", \"coef\": 6.871428708373358}, {\"intent\": \"commandstop\", \"feature\": \"cancel\", \"coef\": 6.586560003727833}, {\"intent\": \"calendar_remove\", \"feature\": \"cancel\", \"coef\": 5.357456387611517}, {\"intent\": \"alarm_set\", \"feature\": \"wake\", \"coef\": 4.421118344443312}, {\"intent\": \"lists_remove\", \"feature\": \"rid\", \"coef\": 3.1329857213334216}, {\"intent\": \"volume_mute\", \"feature\": \"off\", \"coef\": 3.1208117157452873}, {\"intent\": \"alarm_query\", \"feature\": \"wake\", \"coef\": 1.6901281987198629}, {\"intent\": \"wemo_off\", \"feature\": \"disable\", \"coef\": 1.5901633531142816}], \"correct_utterance_example\": [\"alarm_remove\", \"cancel my [time : seven am] alarm\", [[\"cancel\", 3.2256674655709054], [\"my\", 0.8476517195850248], [\"seven\", 0.4819902888610016], [\"am\", 1.5480258081837357], [\"alarm\", 7.8083611347478605]]], \"incorrect_utterance_example\": [\"alarm_set\", \"stop [time : seven am] alarm\", [[\"stop\", 0.6226635805696344], [\"seven\", 0.4819902888610016], [\"am\", 1.5480258081837357], [\"alarm\", 7.8083611347478605]]], \"incorrect_predicted_intents_and_counts\": {\"alarm_set\": 17, \"alarm_query\": 10, \"quirky\": 3, \"commandstop\": 2, \"calendar_remove\": 1, \"negate\": 1, \"lists_remove\": 1}}, \"alarm_set\": {\"f1_score\": 0.8, \"total_count\": 288, \"total_incorrect_count\": 39, \"top_features\": [\"alarm\", \"wake\", \"set\", \"at\", \"an\", \"am\", \"up\", \"five\", \"clock\", \"alert\"], \"overlapping_features\": [{\"intent\": \"alarm_remove\", \"feature\": \"alarm\", \"coef\": 7.8083611347478605}, {\"intent\": \"alarm_query\", \"feature\": \"alarm\", \"coef\": 7.431133772118585}, {\"intent\": \"volume_up\", \"feature\": \"up\", \"coef\": 4.851679794810511}, {\"intent\": \"calendar_set\", \"feature\": \"set\", \"coef\": 4.48599467132008}, {\"intent\": \"hue_lightup\", \"feature\": \"up\", \"coef\": 4.255044390418477}, {\"intent\": \"calendar_set\", \"feature\": \"at\", \"coef\": 4.185219329969828}, {\"intent\": \"alarm_query\", \"feature\": \"set\", \"coef\": 3.5274047410920835}, {\"intent\": \"maths\", \"feature\": \"five\", \"coef\": 2.8655957501063036}, {\"intent\": \"taxi\", \"feature\": \"an\", \"coef\": 2.519459713954873}, {\"intent\": \"alarm_remove\", \"feature\": \"wake\", \"coef\": 1.908153808615783}, {\"intent\": \"alarm_query\", \"feature\": \"wake\", \"coef\": 1.6901281987198629}], \"correct_utterance_example\": [\"alarm_set\", \"wake me up at [time : five am] [date : this week]\", [[\"wake\", 4.421118344443312], [\"me\", 1.6738638137603432], [\"up\", 3.1067164785331904], [\"at\", 3.9763616554950896], [\"five\", 2.4767053018006884], [\"am\", 3.126186716614147], [\"this\", -0.1220186583896435], [\"week\", 0.004228805437778025]]], \"incorrect_utterance_example\": [\"calendar_set\", \"alert me at [time : three pm] to goto the [event_name : concert]\", [[\"alert\", 2.172404213525288], [\"me\", 1.6738638137603432], [\"at\", 3.9763616554950896], [\"three\", 0.9763871106072561], [\"pm\", 1.9009713932764774], [\"to\", -0.06011840794553004], [\"goto\", 0.5480550512837168], [\"the\", -1.4474019206580893], [\"concert\", 0.35618346998052103]]], \"incorrect_predicted_intents_and_counts\": {\"calendar_set\": 28, \"alarm_remove\": 3, \"datetime_query\": 2, \"calendar_query\": 2, \"sendemail\": 1, \"quirky\": 1, \"alarm_query\": 1, \"game\": 1}}, \"alarm_query\": {\"f1_score\": 0.81, \"total_count\": 200, \"total_incorrect_count\": 36, \"top_features\": [\"alarms\", \"alarm\", \"set\", \"have\", \"morning\", \"there\", \"are\", \"wake\", \"time\", \"checkout\"], \"overlapping_features\": [{\"intent\": \"datetime_query\", \"feature\": \"time\", \"coef\": 9.533518313252761}, {\"intent\": \"alarm_set\", \"feature\": \"alarm\", \"coef\": 8.851573863665756}, {\"intent\": \"alarm_remove\", \"feature\": \"alarm\", \"coef\": 7.8083611347478605}, {\"intent\": \"alarm_remove\", \"feature\": \"alarms\", \"coef\": 5.306331850253167}, {\"intent\": \"calendar_set\", \"feature\": \"set\", \"coef\": 4.48599467132008}, {\"intent\": \"calendar_query\", \"feature\": \"have\", \"coef\": 4.440356922684058}, {\"intent\": \"alarm_set\", \"feature\": \"wake\", \"coef\": 4.421118344443312}, {\"intent\": \"alarm_set\", \"feature\": \"set\", \"coef\": 4.1088437596353975}, {\"intent\": \"convert\", \"feature\": \"time\", \"coef\": 3.0837542267884235}, {\"intent\": \"traffic\", \"feature\": \"there\", \"coef\": 2.2912682411926903}, {\"intent\": \"lists_query\", \"feature\": \"are\", \"coef\": 2.1182583396780212}, {\"intent\": \"alarm_remove\", \"feature\": \"wake\", \"coef\": 1.908153808615783}, {\"intent\": \"traffic\", \"feature\": \"are\", \"coef\": 1.7984194836049188}], \"correct_utterance_example\": [\"alarm_query\", \"what alarms i have set\", [[\"what\", 1.661098375562258], [\"alarms\", 10.161784835394235], [\"i\", 0], [\"have\", 2.7576260278664493], [\"set\", 3.5274047410920835]]], \"incorrect_utterance_example\": [\"alarm_set\", \"did i set an alarm to [alarm_type : wake up] in the [timeofday : morning]\", [[\"did\", 1.0677178377523133], [\"i\", 0], [\"set\", 3.5274047410920835], [\"an\", -0.32083835676282274], [\"alarm\", 7.431133772118585], [\"to\", -0.35061986324699823], [\"wake\", 1.6901281987198629], [\"up\", 1.133180905360718], [\"in\", -0.48142941717875315], [\"the\", 0.7710596453646386], [\"morning\", 2.2904356408431785]]], \"incorrect_predicted_intents_and_counts\": {\"alarm_set\": 22, \"calendar_set\": 3, \"calendar_query\": 3, \"quirky\": 3, \"datetime_query\": 2, \"takeaway_query\": 1, \"lists_query\": 1, \"email_query\": 1}}}))\n        });\n        renderjson.set_show_to_level(1)\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "incorrect_predicted_intents_report = MacroIntentRefinement.get_incorrect_predicted_intents_report(\n",
    "    domain_df, incorrect_intent_predictions_df, intent_report_df, intent_feature_rank_df)\n",
    "\n",
    "# TODO: Remove unused measures.\n",
    "RenderJSON(incorrect_predicted_intents_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to save the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataUtils.save_json(incorrect_predicted_intents_report, 'data/reports/' +\n",
    "                    domain_selection + '_incorrect_predicted_intents_report.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add in way to show the improvements when refinement is complete (save original json and new json as one file with two main keys), how best to compare them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we will save a dictionary of all of the incorrectly predicted intents for each intent to refine in this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_refinement_dictionary = MacroIntentRefinement.get_intent_dataframes_to_refine(\n",
    "    incorrect_intent_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent refinement: human in the for loop.\n",
    "\n",
    "<span style=\"color: red;\">For changing to another intent to refine within the same domain, start here again.</span>\n",
    "\n",
    "Now it's your turn to shine human!\n",
    "\n",
    "You will provide a refinement to each incorrectly predicted intent. some of the incorrectly predicted utterances are actually fine the way they are, you may need to review the intent that is fasly being predicted...\n",
    "\n",
    "\n",
    "Besides correcting the utterances(e.g. spelling or grammar), you can also mark an entry with the following:\n",
    "* **review**: the utterance needs to be reviewed again by a human\n",
    "* **move**: the utterance needs to be moved to another intent(NOTE: if you have a big data set, it might be better to just **remove** the utterance from the data set)\n",
    "* **remove**: the utterance should be removed from the dataset\n",
    "\n",
    "You can use your human ability to refine the NLU intent data by answering the following questions:\n",
    "\n",
    "0. What should this intent actually do in general? There is usually a specific action the intent should do, and some of the utterances are mislabeled or are not specific enough. If you can't exactly place the utterance without any context (just the untagged utterance by itself), then an AI can't either. \n",
    "\n",
    "1. Does the utterance fit to the intent? -> mark as move, remove, and/or review\n",
    "\n",
    "2. Is the utterance grammar or spelling wrong but(1) is fine? -> correct the utterance\n",
    "\n",
    "3. Is this intent collidating with another intent because the scope of both intents are overlapping? -> redefine the scope of the intents (either combine them or separate their functionality better)\n",
    "\n",
    "4. Is the intent collidating with another intent because certain keywords overlap between intents? -> redefine the keywords to split between intents or merge them together if they are similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of intents, pick one to refine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_to_refine = MacroIntentRefinement.list_and_select_intent(incorrect_intent_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to take a quick second look at entries that are predicted correctly, to see what this intent is supposed to be doing (but perhaps even some of these are wrong too, LOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>answer_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>cancel my seven am alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>cancel my [time : seven am] alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>remove the alarm set for ten pm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>remove the alarm set for [time : ten pm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>remove the alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>remove the alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>remove the first alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>remove the first alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>disable the first alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>disable the first alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>delete the first alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>delete the first alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>turn off eight am alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>turn off [time : eight am] alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>cancel my eight am alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>cancel my [time : eight am] alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>get rid of my eight am alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>get rid of my [time : eight am] alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>remove wake up calls for this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>remove [alarm_type : wake up] calls for [date ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "42   Write what you would tell your PDA in the foll...   \n",
       "43   Write what you would tell your PDA in the foll...   \n",
       "180  How would you ask your PDA to remove an alarm ...   \n",
       "385  How would you ask your PDA to remove an alarm ...   \n",
       "386  How would you ask your PDA to remove an alarm ...   \n",
       "387  How would you ask your PDA to remove an alarm ...   \n",
       "568  How would you ask your PDA to remove an alarm ...   \n",
       "569  How would you ask your PDA to remove an alarm ...   \n",
       "570  How would you ask your PDA to remove an alarm ...   \n",
       "877  How would you ask your PDA to remove an alarm ...   \n",
       "\n",
       "                      answer_normalised scenario        intent  \\\n",
       "42             cancel my seven am alarm    alarm  alarm_remove   \n",
       "43      remove the alarm set for ten pm    alarm  alarm_remove   \n",
       "180                    remove the alarm    alarm  alarm_remove   \n",
       "385              remove the first alarm    alarm  alarm_remove   \n",
       "386             disable the first alarm    alarm  alarm_remove   \n",
       "387              delete the first alarm    alarm  alarm_remove   \n",
       "568             turn off eight am alarm    alarm  alarm_remove   \n",
       "569            cancel my eight am alarm    alarm  alarm_remove   \n",
       "570        get rid of my eight am alarm    alarm  alarm_remove   \n",
       "877  remove wake up calls for this week    alarm  alarm_remove   \n",
       "\n",
       "    predicted_label                                  answer_annotation  \n",
       "42     alarm_remove                  cancel my [time : seven am] alarm  \n",
       "43     alarm_remove           remove the alarm set for [time : ten pm]  \n",
       "180    alarm_remove                                   remove the alarm  \n",
       "385    alarm_remove                             remove the first alarm  \n",
       "386    alarm_remove                            disable the first alarm  \n",
       "387    alarm_remove                             delete the first alarm  \n",
       "568    alarm_remove                   turn off [time : eight am] alarm  \n",
       "569    alarm_remove                  cancel my [time : eight am] alarm  \n",
       "570    alarm_remove              get rid of my [time : eight am] alarm  \n",
       "877    alarm_remove  remove [alarm_type : wake up] calls for [date ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_df[(domain_df['intent'] == intent_to_refine) & (\n",
    "    domain_df['intent'] == domain_df['predicted_label'])].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">In rare occasions, you may want to remove the whole intent from the data set. This can be useful when the whole intent doesn't make sense or is out of scope. Run this and the next cell ONLY if you are sure you want to remove the whole intent, not just the incorrect predictions! Then start on the next domain again further above. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = MacroIntentRefinement.remove_intent(nlu_data_df, intent_to_refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">Save the state of the data set with the removed intent, and then start again on the next intent or domain at the top.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('data/refined/nlu_data_refined_df.csv')\n",
    "nlu_data_df = pd.read_csv(\n",
    "    'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refine the incorrect predictions of this intent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad293c015eb24f83913275e0e5461066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sheet(cells=(Cell(column_end=0, column_start=0, numeric_format=None, row_end=34, row_start=0, squeeze_row=Falsâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_review_sheet = MacroDataRefinement.create_sheet(\n",
    "    intent_refinement_dictionary[intent_to_refine])\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the reviewed data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = MacroDataRefinement.convert_sheet_to_dataframe(\n",
    "    to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df.to_csv(\n",
    "    'data/reviewed/reviewed_'+ domain_selection + '_' + intent_to_refine + '_incorrectly_predicted_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove this when done.\n",
    "reviewed_intent_df = pd.read_csv(\n",
    "    'data/reviewed/reviewed_' + domain_selection + '_' + intent_to_refine + '_incorrectly_predicted_df.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For entries that have been marked `move`, we will need to know the intent that they are moving to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add in list of intents above this cell so people can see if it's the right intent to pick\n",
    "#TODO: BUG: change method to look up scenario for changed intent to relabel the scenario\n",
    "refined_intent_df = reviewed_intent_df.apply(\n",
    "    MacroDataRefinement.move_entry, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably a good idea to save this in a csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df.to_csv(\n",
    "    'data/refined/refined_' + domain_selection + '_' + intent_to_refine + '_incorrectly_predicted_df.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove this when done.\n",
    "refined_intent_df = pd.read_csv(\n",
    "    'data/refined/refined_' + domain_selection + '_' + intent_to_refine + '_incorrectly_predicted_df.csv', sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mark all the refined entries and merge these with the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated dataframe\n"
     ]
    }
   ],
   "source": [
    "refined_intent_df = MacroDataRefinement.mark_entries_as_refined(refined_dataframe=refined_intent_df, refined_type='intent')\n",
    "\n",
    "updated_df = MacroDataRefinement.update_dataframe(\n",
    "    nlu_data_df, refined_intent_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to double check your data set after you have refined it before saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25074, 15)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure the intents you just refined are there. This just checks the first 10, but that is good enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>answerid</th>\n",
       "      <th>notes</th>\n",
       "      <th>question</th>\n",
       "      <th>suggested_entities</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>intent_refined</th>\n",
       "      <th>entity_refined</th>\n",
       "      <th>remove</th>\n",
       "      <th>status</th>\n",
       "      <th>answer_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>Stop 7am alarm</td>\n",
       "      <td>stop seven am alarm</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stop [time : seven am] alarm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>Delete the alarm I just set</td>\n",
       "      <td>delete the alarm i just set</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>delete the alarm i just set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>90.0</td>\n",
       "      <td>3532.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>remove wake-up calls for this week</td>\n",
       "      <td>remove wake up calls for this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remove [alarm_type : wake up] calls for [date ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>90.0</td>\n",
       "      <td>3533.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>cancel wake-up calls for this week</td>\n",
       "      <td>cancel wake up calls for this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancel [alarm_type : wake up] calls for [date ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>90.0</td>\n",
       "      <td>3534.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>stop wake-up calls for this week</td>\n",
       "      <td>stop wake up calls for this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stop [alarm_type : wake up] calls for [date : ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>96.0</td>\n",
       "      <td>3684.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>Turn off any morning alarms after Friday</td>\n",
       "      <td>turn off any morning alarms after Friday</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>turn off any [timeofday : morning] alarms [dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>150.0</td>\n",
       "      <td>4504.0</td>\n",
       "      <td>split</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>2AM REMOVED 4PM SET</td>\n",
       "      <td>two am removed</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>quirky</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>MOD</td>\n",
       "      <td>[time : two am] removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>150.0</td>\n",
       "      <td>4505.0</td>\n",
       "      <td>split</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>12AM REMOVED 2PM SET</td>\n",
       "      <td>twelve am removed</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>quirky</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>MOD</td>\n",
       "      <td>[time : twelve am] removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>150.0</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>split</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>1PM REMOVED 1AM SET</td>\n",
       "      <td>one pm removed</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>MOD</td>\n",
       "      <td>[time : one pm] removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>215.0</td>\n",
       "      <td>5807.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>Change the alarm weekly settings</td>\n",
       "      <td>change the alarm weekly settings</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>change the alarm weekly settings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid  answerid  notes  \\\n",
       "44       2.0     560.0    NaN   \n",
       "179      9.0    1530.0    NaN   \n",
       "877     90.0    3532.0    NaN   \n",
       "878     90.0    3533.0    NaN   \n",
       "879     90.0    3534.0    NaN   \n",
       "942     96.0    3684.0    NaN   \n",
       "1255   150.0    4504.0  split   \n",
       "1257   150.0    4505.0  split   \n",
       "1259   150.0    4506.0  split   \n",
       "1565   215.0    5807.0    NaN   \n",
       "\n",
       "                                               question suggested_entities  \\\n",
       "44    Write what you would tell your PDA in the foll...         date, time   \n",
       "179   How would you ask your PDA to remove an alarm ...         date, time   \n",
       "877   How would you ask your PDA to remove an alarm ...         date, time   \n",
       "878   How would you ask your PDA to remove an alarm ...         date, time   \n",
       "879   How would you ask your PDA to remove an alarm ...         date, time   \n",
       "942   How would you ask your PDA to remove an alarm ...         date, time   \n",
       "1255  How would you ask your PDA to remove an alarm ...         date, time   \n",
       "1257  How would you ask your PDA to remove an alarm ...         date, time   \n",
       "1259  How would you ask your PDA to remove an alarm ...         date, time   \n",
       "1565  How would you ask your PDA to remove an alarm ...         date, time   \n",
       "\n",
       "                                        answer  \\\n",
       "44                              Stop 7am alarm   \n",
       "179                Delete the alarm I just set   \n",
       "877         remove wake-up calls for this week   \n",
       "878         cancel wake-up calls for this week   \n",
       "879           stop wake-up calls for this week   \n",
       "942   Turn off any morning alarms after Friday   \n",
       "1255                       2AM REMOVED 4PM SET   \n",
       "1257                      12AM REMOVED 2PM SET   \n",
       "1259                       1PM REMOVED 1AM SET   \n",
       "1565          Change the alarm weekly settings   \n",
       "\n",
       "                             answer_normalised scenario        intent  \\\n",
       "44                         stop seven am alarm    alarm  alarm_remove   \n",
       "179                delete the alarm i just set    alarm  alarm_remove   \n",
       "877         remove wake up calls for this week    alarm  alarm_remove   \n",
       "878         cancel wake up calls for this week    alarm  alarm_remove   \n",
       "879           stop wake up calls for this week    alarm  alarm_remove   \n",
       "942   turn off any morning alarms after Friday    alarm  alarm_remove   \n",
       "1255                            two am removed    alarm  alarm_remove   \n",
       "1257                         twelve am removed    alarm  alarm_remove   \n",
       "1259                            one pm removed    alarm  alarm_remove   \n",
       "1565          change the alarm weekly settings    alarm  alarm_remove   \n",
       "\n",
       "     predicted_label intent_refined  entity_refined remove status  \\\n",
       "44         alarm_set           True             NaN   True    NaN   \n",
       "179        alarm_set           True             NaN   True    NaN   \n",
       "877     alarm_remove           True             NaN  False    NaN   \n",
       "878     alarm_remove           True             NaN   True    NaN   \n",
       "879        alarm_set           True             NaN   True    NaN   \n",
       "942      alarm_query           True             NaN  False    NaN   \n",
       "1255          quirky           True             NaN   True    MOD   \n",
       "1257          quirky           True             NaN   True    MOD   \n",
       "1259    alarm_remove           True             NaN   True    MOD   \n",
       "1565    alarm_remove           True             NaN   True    NaN   \n",
       "\n",
       "                                      answer_annotation  \n",
       "44                         stop [time : seven am] alarm  \n",
       "179                         delete the alarm i just set  \n",
       "877   remove [alarm_type : wake up] calls for [date ...  \n",
       "878   cancel [alarm_type : wake up] calls for [date ...  \n",
       "879   stop [alarm_type : wake up] calls for [date : ...  \n",
       "942   turn off any [timeofday : morning] alarms [dat...  \n",
       "1255                            [time : two am] removed  \n",
       "1257                         [time : twelve am] removed  \n",
       "1259                            [time : one pm] removed  \n",
       "1565                   change the alarm weekly settings  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df[(updated_df['intent']== intent_to_refine) & (updated_df['intent_refined']== True)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of entries refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2526, 15)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df[updated_df['intent_refined']== True].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of entries to be reviewed (excluding removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 15)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df[(updated_df['intent'] != updated_df['predicted_label'])\n",
    "           & (updated_df['intent_refined'].isna()) & (updated_df['remove'].isna())].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are sure the numbers are correct, save the `updated_df` to a csv and reload it in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('data/refined/nlu_data_refined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = pd.read_csv('data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\"> Stop here and repeat the loops for the next intent or domain until you have done them all. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the last little bit!\n",
    "\n",
    "Run this once you have refined all of the intents for all of the domains to see if there is anything left over to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_to_refine_df = updated_df[(updated_df['intent'] != updated_df['predicted_label'])\n",
    "           & (updated_df['intent_refined'].isna()) & (updated_df['remove'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_to_refine_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_to_refine_df = DataUtils.prepare_dataframe_for_refinement(batch_to_refine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review_sheet = MacroDataRefinement.create_sheet(\n",
    "    batch_to_refine_df)\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = MacroDataRefinement.convert_sheet_to_dataframe(\n",
    "    to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df.to_csv('data/reviewed/reviewed_misc_domains_and_intents_to_sort_incorrectly_predicted_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save the reviewed intents to the correct csvs that already exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Refactor this into a class. This is a bit of a hack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = reviewed_intent_df.intent.unique().tolist()\n",
    "\n",
    "domains = [domain for intent in intents for domain in reviewed_intent_df[reviewed_intent_df.intent == intent]['scenario'].unique()]\n",
    "\n",
    "intents_domains = zip(intents, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent, domain in intents_domains:\n",
    "    reviewed_intent_df[reviewed_intent_df.intent == intent].to_csv('data/reviewed/reviewed_' + domain + '_' + intent + '_incorrectly_predicted_df.csv', mode='a', header=False)\n",
    "    print(intent, domain)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = reviewed_intent_df.apply(\n",
    "    MacroDataRefinement.move_entry, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = MacroDataRefinement.mark_entries_as_refined(\n",
    "    refined_dataframe=refined_intent_df, refined_type='intent')\n",
    "\n",
    "updated_df = MacroDataRefinement.update_dataframe(\n",
    "    nlu_data_df, refined_intent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df[(updated_df['intent'] != updated_df['predicted_label'])\n",
    "           & (updated_df['intent_refined'].isna()) & (updated_df['remove'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('data/refined/nlu_data_refined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_refined_df = pd.read_csv('data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Improve flow between a domain and refining within the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_refined_df[nlu_data_refined_df['remove'] != True]\n",
    "\n",
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=removed_nlu_data_refined_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_intent_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=removed_nlu_data_refined_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "improved_intent_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor from here using the method for plotting, write a new method that can do the side-by-side plot.\n",
    "# TODO: also do the plot for domains!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_improved_intent_report_df = improved_intent_report_df.drop(improved_intent_report_df.tail(3).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_improved_intent_report_df.sort_values(by='f1-score', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_intent_report_df = intent_report_df.drop(intent_report_df.tail(3).index)\n",
    "#graph_intent_report_df.sort_values(by='f1-score', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_refinement_benchmark_df = pd.DataFrame.merge(graph_intent_report_df, graph_improved_intent_report_df, on='intent', how='outer', suffixes=('_original', '_refined'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_refinement_benchmark_df.sort_values(\n",
    "    by='f1-score_refined', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_refinement_benchmark_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: improve layout of graph\n",
    "#TODO: side-by-side bar chart of the original and refined dataframes. \n",
    "\n",
    "#fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "y_axis = np.arange(len(intent_refinement_benchmark_df.intent))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "original = ax.barh(y_axis - width/2, intent_refinement_benchmark_df['f1-score_original'], width, label='Original', color='#1f77b4')\n",
    "\n",
    "refined = ax.barh(y_axis + width/2, intent_refinement_benchmark_df['f1-score_refined'], width, label='Refined', color='#ff7f0e')\n",
    "\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax.set_xlabel(\"f1-score\")\n",
    "ax.set_ylabel(\"Intent\")\n",
    "ax.set_yticks(y_axis, intent_refinement_benchmark_df.intent)\n",
    "ax.set_title(\"f1 scores by intent\")\n",
    "\n",
    "fig.tight_layout()\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "#TODO: also do this with domains.\n",
    "#TODO: review lowest 100 scoring intents. Which intents are the going to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MacroDataRefinement.get_incorrect_predicted_intents_report(\n",
    "    removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == domain_selection], refined_incorrect_intent_predictions_df, improved_intent_report_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here everything needs to be refactored\n",
    "\n",
    "We will get the probabilities of each intent from the intent classifier and append that to our dataframe. Then we will review the 250 (or more??) lowest ranking entries and see if they are actually correct (spoiler alert: most aren't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df['intent_probability'] = removed_nlu_data_refined_df['answer_normalised'].apply(\n",
    "    lambda x: IntentMatcher.get_prediction_probability(LR_intent_classifier_model, tfidf_vectorizer, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: review the weakest 100 in a sheet.\n",
    "low_probability_to_refine_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['intent_refined'] != True].sort_values(by='intent_probability', ascending=True).head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_probability_to_review_df = low_probability_to_refine_df.drop(\n",
    "    columns=[\n",
    "        'userid', 'notes', 'answer', 'answerid', 'suggested_entities', 'intent_refined', 'remove', 'status', 'intent_probability', 'entity_refined'\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review_sheet = MacroDataRefinement.create_sheet(\n",
    "    low_probability_to_review_df)\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = MacroDataRefinement.convert_sheet_to_dataframe(\n",
    "    to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: export reviewed_intent_df to a csv file and integrate it into the nlu_data_refined_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df.to_csv('data/reviewed/reviewed_low_scoring_intents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = reviewed_intent_df.apply(\n",
    "    MacroDataRefinement.move_entry, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df.to_csv(\n",
    "    'data/refined/refined_low_scoring_intents_df.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = MacroDataRefinement.mark_entries_as_refined(\n",
    "    refined_dataframe=refined_intent_df, refined_type='intent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    updated_df = MacroDataRefinement.update_dataframe(\n",
    "        nlu_data_refined_df, refined_intent_df)\n",
    "    print('updated refined df')\n",
    "except:\n",
    "    updated_df = MacroDataRefinement.update_dataframe(\n",
    "        upgraded_df, refined_intent_df)\n",
    "    print('updated upgraded df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('data/refined/nlu_data_refined_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df[updated_df.index == 1259]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_intent_classifier_model.predict_proba(tfidf_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df['intent_probability'] = removed_nlu_data_refined_df['answer_normalised'].apply(LR.predict_proba, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides some incorrect utterances and intents, we can see that there is an overlap between the intent 'alarm_set' and the intent 'calandar_set'. This is because those two intents are not well defined and will require refinement. We will try to fix this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: integrate refined_intent_df into the main dataset and save it as nlu_data_refined_df\n",
    "\n",
    "\n",
    "#TODO: export nlu_data_refined_df to a csv file and save it as NLU-Data-Home-Domain-Annotated-Refined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for every intent in the predicted intent column, get the top 5 tfidf features and their scores\n",
    "# Like this: https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "#TODO: Make sure to pass them to the intent refinement process for each intent by putting them in the report!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: get the counts of the terms from the utterances that are incorrect for a specific domain (should I filter by tfidf scores?)\n",
    "#TODO: Look up the most popular terms for an intent if they are red or green for that intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word (feature) in the utterances, we get the coeficients for the intents.\n",
    "# From the shape, we see it contains the classes and the coeficients.\n",
    "coefs = LR_intent_classifier_model.coef_\n",
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu_engine import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the encoded classes\n",
    "classes = LR_intent_classifier_model.classes_\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cant to get the actual feature names (the words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try an example with TFIDF only, this only tells us overall the TFIDF score for each word, not related to the intent\n",
    "from nlu_engine import TfidfEncoder\n",
    "\n",
    "utterance = 'turn off the alarm I set'\n",
    "\n",
    "response = TfidfEncoder.encode_vectors(\n",
    "    utterance, tfidf_vectorizer)\n",
    "\n",
    "for vector in response.nonzero()[1]:\n",
    "    print(f'word: {feature_names[vector]} - ranking: {response[0, vector]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rip out a list of tuples for the features and their coeficients for the intent\n",
    "output = []\n",
    "for classIndex, features in enumerate(coefs):\n",
    "    for featureIndex, feature in enumerate(features):\n",
    "        output.append(\n",
    "            (classes[classIndex], feature_names[featureIndex], feature))\n",
    "feature_rank_df = pd.DataFrame(output, columns=['class', 'feature', 'coef'])\n",
    "feature_rank_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a good idea to convert the classes from the encoded to a normal human form\n",
    "feature_rank_df['class'] = LabelEncoder.inverse_transform(feature_rank_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the features by the absolute value of their coefficient and color them red or green\n",
    "feature_rank_df[\"abs_value\"] = feature_rank_df[\"coef\"].apply(lambda x: abs(x))\n",
    "feature_rank_df[\"colors\"] = feature_rank_df[\"coef\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "feature_rank_df = feature_rank_df.sort_values(\"abs_value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at an example of the word 'set'\n",
    "feature_rank_df[(feature_rank_df['feature'] == 'set') & (feature_rank_df['colors'] == 'red')\n",
    "   ].sort_values('abs_value', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity extraction report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement brown clustering to improve entity extraction (see entity_extractor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(data_df=nlu_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Benchmark the state features to find the best and the worst, remove/replace worst: add in state features like here: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
    "# Specifically, we want print_state_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from the entity extraction report, the entity extraction is not working for the alarm_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: move this below the intent cleaning flow\n",
    "nlu_scenario_df = nlu_scenario_df[nlu_scenario_df['answer_annotation'].str.contains(\n",
    "    'alarm_type')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Convert to ipysheet and review\n",
    "TODO: add in description of the types of fixes we can do to the NLU data for entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: same as above for intents but with predicted entities: report on them, break them down into a dictionary of dataframes and refine them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with 'alarm' and the alarm_type:\n",
    "* We see that the alarm_type entities are really event_name(ie wake up, soccer practice) except for ID 5879, we will need to change them to event_name and remove ID 5879.\n",
    "* The last one(ID 6320) is a mistake. Someone got confused with the prompt and assumed alarm is a security system. This is out of scope for the alarm domain, as the alarms are ones set on a phone or other device. We will drop this utterance.\n",
    "Once you are done reviewing, you convert it back to a dataframe and check to make sure it looks okay.\n",
    "Let's change all alarm_type entities to event_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewed_scenario_df['answer_annotation'] = reviewed_scenario_df['answer_annotation'].str.replace(\n",
    "    'alarm_type', 'event_name')\n",
    "reviewed_scenario_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay dokey, now we can merge this with the original data set and see if it made a difference already(well of course it did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df.drop(\n",
    "    reviewed_scenario_df[reviewed_scenario_df['remove'] == True].index, inplace=True)\n",
    "\n",
    "reviewed_scenario_df = reviewed_scenario_df[~reviewed_scenario_df['remove'] == True]\n",
    "\n",
    "nlu_data_df.loc[nlu_data_df.index.intersection(\n",
    "    reviewed_scenario_df.index), 'answer_annotation'] = reviewed_scenario_df['answer_annotation']\n",
    "\n",
    "nlu_data_df[(nlu_data_df['scenario'].str.contains('alarm')) & (nlu_data_df['answer_annotation'].str.contains(\n",
    "    'event_name'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark changed data set\n",
    "TODO: repeat reports for the changed data set for domain and entities and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_reviewed_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=nlu_data_df)\n",
    "entity_reviewed_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure it is okay, you can save it as a csv file, make sure to name it correctly(i.e. `alarm_domain_first_review.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_scenario_df.to_csv('alarm_domain_first_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back up and check to make sure it looks okay. Make sure to give it the right name!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_domain_first_review_df = pd.read_csv(\n",
    "    'alarm_domain_first_review.csv', index_col=0)\n",
    "audio_domain_first_review_df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the evaluate_classifier in the NLU engine to check f1 score for intents and entities in the domain vs original NLU data of domain!\n",
    "# Value: benchmark!\n",
    "#TODO: implement a flow for getting the domains with the lowest f1 scores by intent/domain and entities and cleaning them by the order of the lowest f1 scores\n",
    "# TODO: concat all reviewed dfs and save to csv\n",
    "# TODO: add benchmark for whole NLU data set before and after cleaning! (by intents and domains!)\n",
    "# TODO: review the review marked entries\n",
    "# TODO: add new column for notes\n",
    "# TODO: change flow of review for only ones that should be reviewed, not all of the ones that have been changed (track changes by comparing against the original data set)\n",
    "# TODO: do the changed utterances have to be changed in other fields too or is it just enough for the tagged utterancve field?\n",
    "# TODO: add visualizations of domains, their intents, keywords in utterances, and entities to top\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

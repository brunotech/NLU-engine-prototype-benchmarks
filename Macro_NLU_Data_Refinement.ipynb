{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipysheet\n",
    "import nltk\n",
    "\n",
    "from utils.nlu_engine import NLUEngine\n",
    "from utils.nlu_engine import MacroDataRefinement\n",
    "from utils.nlu_engine import DataUtils\n",
    "from utils.nlu_engine import IntentMatcher, LR\n",
    "from utils.nlu_engine import EntityExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, youâ€™ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "NOTE: We will use Logistic Regression with TFIDF features to train our intent models and CRFs for entity exraction. Why? Well, they are very fast and both methods aren't state-of-the-art. This is good, because it is easier to find problems we will need to refine in the dataset than if we were to use a proper NLU engine like Snips or something SOTA like BERT. It is very important to note that some of the the problems we will pick up on, might not be an actual issue, but might be due to the limitations of the models. Refining the real problems and ignoring the limitations of the models is a good way to improve the models. Then when the dataset is ready, we can use some more advanced NLU engine and get the best performance possible.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Refactor with text inputs for increased flexibility like this: https://queirozf.com/entries/interactive-controls-for-jupyter-notebooks-python-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = DataUtils.load_data(\n",
    "    'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intent classifier report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a report by domain classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression(random_state=0, solver='liblinear')\n",
      "Cross validating with LogisticRegression(random_state=0, solver='liblinear')\n",
      "Time it took to cross validate LogisticRegression(random_state=0, solver='liblinear'): 4.389686346054077\n",
      "Generating report for LogisticRegression(random_state=0, solver='liblinear')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/utils/nlu_engine/analytics.py:44: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['classifier'] = df['classifier'].str.replace(r\"\\([^()]*\\)\", \"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alarm</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.899174</td>\n",
       "      <td>0.942808</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audio</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calendar</td>\n",
       "      <td>0.841991</td>\n",
       "      <td>0.927768</td>\n",
       "      <td>0.882801</td>\n",
       "      <td>2935.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cooking</td>\n",
       "      <td>0.938144</td>\n",
       "      <td>0.675743</td>\n",
       "      <td>0.785612</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datetime</td>\n",
       "      <td>0.849254</td>\n",
       "      <td>0.816356</td>\n",
       "      <td>0.832480</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>email</td>\n",
       "      <td>0.955677</td>\n",
       "      <td>0.912993</td>\n",
       "      <td>0.933848</td>\n",
       "      <td>1724.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>general</td>\n",
       "      <td>0.815404</td>\n",
       "      <td>0.832813</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>6089.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iot</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>0.936728</td>\n",
       "      <td>0.953653</td>\n",
       "      <td>1296.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lists</td>\n",
       "      <td>0.924945</td>\n",
       "      <td>0.881178</td>\n",
       "      <td>0.902531</td>\n",
       "      <td>951.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>music</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.660617</td>\n",
       "      <td>0.763103</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news</td>\n",
       "      <td>0.869388</td>\n",
       "      <td>0.744755</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>858.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>play</td>\n",
       "      <td>0.889581</td>\n",
       "      <td>0.855831</td>\n",
       "      <td>0.872379</td>\n",
       "      <td>2504.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qa</td>\n",
       "      <td>0.660120</td>\n",
       "      <td>0.887248</td>\n",
       "      <td>0.757015</td>\n",
       "      <td>2235.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>recommendation</td>\n",
       "      <td>0.839442</td>\n",
       "      <td>0.705279</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>social</td>\n",
       "      <td>0.957411</td>\n",
       "      <td>0.793785</td>\n",
       "      <td>0.867954</td>\n",
       "      <td>708.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>takeaway</td>\n",
       "      <td>0.936416</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>transport</td>\n",
       "      <td>0.945571</td>\n",
       "      <td>0.893145</td>\n",
       "      <td>0.918611</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>weather</td>\n",
       "      <td>0.918511</td>\n",
       "      <td>0.885548</td>\n",
       "      <td>0.901728</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.897621</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.855921</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.863054</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.854831</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            domain  precision    recall  f1-score       support  \\\n",
       "0            alarm   0.990893  0.899174  0.942808    605.000000   \n",
       "1            audio   0.950000  0.756219  0.842105    402.000000   \n",
       "2         calendar   0.841991  0.927768  0.882801   2935.000000   \n",
       "3          cooking   0.938144  0.675743  0.785612    404.000000   \n",
       "4         datetime   0.849254  0.816356  0.832480    697.000000   \n",
       "5            email   0.955677  0.912993  0.933848   1724.000000   \n",
       "6          general   0.815404  0.832813  0.824017   6089.000000   \n",
       "7              iot   0.971200  0.936728  0.953653   1296.000000   \n",
       "8            lists   0.924945  0.881178  0.902531    951.000000   \n",
       "9            music   0.903226  0.660617  0.763103    551.000000   \n",
       "10            news   0.869388  0.744755  0.802260    858.000000   \n",
       "11            play   0.889581  0.855831  0.872379   2504.000000   \n",
       "12              qa   0.660120  0.887248  0.757015   2235.000000   \n",
       "13  recommendation   0.839442  0.705279  0.766534    682.000000   \n",
       "14          social   0.957411  0.793785  0.867954    708.000000   \n",
       "15        takeaway   0.936416  0.790244  0.857143    410.000000   \n",
       "16       transport   0.945571  0.893145  0.918611    992.000000   \n",
       "17         weather   0.918511  0.885548  0.901728   1031.000000   \n",
       "18        accuracy   0.853673  0.853673  0.853673      0.853673   \n",
       "19       macro avg   0.897621  0.825301  0.855921  25074.000000   \n",
       "20    weighted avg   0.863054  0.853673  0.854831  25074.000000   \n",
       "\n",
       "            classifier encoding  \n",
       "0   LogisticRegression    tfidf  \n",
       "1   LogisticRegression    tfidf  \n",
       "2   LogisticRegression    tfidf  \n",
       "3   LogisticRegression    tfidf  \n",
       "4   LogisticRegression    tfidf  \n",
       "5   LogisticRegression    tfidf  \n",
       "6   LogisticRegression    tfidf  \n",
       "7   LogisticRegression    tfidf  \n",
       "8   LogisticRegression    tfidf  \n",
       "9   LogisticRegression    tfidf  \n",
       "10  LogisticRegression    tfidf  \n",
       "11  LogisticRegression    tfidf  \n",
       "12  LogisticRegression    tfidf  \n",
       "13  LogisticRegression    tfidf  \n",
       "14  LogisticRegression    tfidf  \n",
       "15  LogisticRegression    tfidf  \n",
       "16  LogisticRegression    tfidf  \n",
       "17  LogisticRegression    tfidf  \n",
       "18  LogisticRegression    tfidf  \n",
       "19  LogisticRegression    tfidf  \n",
       "20  LogisticRegression    tfidf  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_labels = 'scenario'\n",
    "\n",
    "domain_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict=domain_labels,\n",
    "    classifier=LR\n",
    ")\n",
    "\n",
    "domain_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's do a report by intent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression(random_state=0, solver='liblinear')\n",
      "Cross validating with LogisticRegression(random_state=0, solver='liblinear')\n",
      "Time it took to cross validate LogisticRegression(random_state=0, solver='liblinear'): 15.826901197433472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for LogisticRegression(random_state=0, solver='liblinear')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/utils/nlu_engine/analytics.py:44: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['classifier'] = df['classifier'].str.replace(r\"\\([^()]*\\)\", \"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affirm</td>\n",
       "      <td>0.988868</td>\n",
       "      <td>0.962094</td>\n",
       "      <td>0.975297</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alarm_query</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.708543</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.670330</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alarm_set</td>\n",
       "      <td>0.773770</td>\n",
       "      <td>0.816609</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>wemo_off</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>0.851374</td>\n",
       "      <td>0.726626</td>\n",
       "      <td>0.768686</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>0.855003</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.835706</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          intent  precision    recall  f1-score       support  \\\n",
       "0     addcontact   0.909091  0.574713  0.704225     87.000000   \n",
       "1         affirm   0.988868  0.962094  0.975297    554.000000   \n",
       "2    alarm_query   0.898089  0.708543  0.792135    199.000000   \n",
       "3   alarm_remove   0.938462  0.521368  0.670330    117.000000   \n",
       "4      alarm_set   0.773770  0.816609  0.794613    289.000000   \n",
       "..           ...        ...       ...       ...           ...   \n",
       "66      wemo_off   0.926829  0.775510  0.844444     98.000000   \n",
       "67       wemo_on   0.981481  0.697368  0.815385     76.000000   \n",
       "68      accuracy   0.833333  0.833333  0.833333      0.833333   \n",
       "69     macro avg   0.851374  0.726626  0.768686  25074.000000   \n",
       "70  weighted avg   0.855003  0.833333  0.835706  25074.000000   \n",
       "\n",
       "            classifier encoding  \n",
       "0   LogisticRegression    tfidf  \n",
       "1   LogisticRegression    tfidf  \n",
       "2   LogisticRegression    tfidf  \n",
       "3   LogisticRegression    tfidf  \n",
       "4   LogisticRegression    tfidf  \n",
       "..                 ...      ...  \n",
       "66  LogisticRegression    tfidf  \n",
       "67  LogisticRegression    tfidf  \n",
       "68  LogisticRegression    tfidf  \n",
       "69  LogisticRegression    tfidf  \n",
       "70  LogisticRegression    tfidf  \n",
       "\n",
       "[71 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "intent_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove next cell and move the one after to the scenario report area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression(random_state=0, solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>abs_value</th>\n",
       "      <th>colors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>200</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>addcontact</td>\n",
       "      <td>29</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486875</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>zone</td>\n",
       "      <td>-0.022681</td>\n",
       "      <td>0.022681</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486876</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>zones</td>\n",
       "      <td>-0.001073</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486877</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>zoo</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486878</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>zucchini</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486879</th>\n",
       "      <td>wemo_on</td>\n",
       "      <td>zydeco</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486880 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             class   feature      coef  abs_value colors\n",
       "0       addcontact        12 -0.000613   0.000613    red\n",
       "1       addcontact        16 -0.000600   0.000600    red\n",
       "2       addcontact       200 -0.000839   0.000839    red\n",
       "3       addcontact        23 -0.000561   0.000561    red\n",
       "4       addcontact        29 -0.000988   0.000988    red\n",
       "...            ...       ...       ...        ...    ...\n",
       "486875     wemo_on      zone -0.022681   0.022681    red\n",
       "486876     wemo_on     zones -0.001073   0.001073    red\n",
       "486877     wemo_on       zoo -0.002001   0.002001    red\n",
       "486878     wemo_on  zucchini -0.000635   0.000635    red\n",
       "486879     wemo_on    zydeco -0.001070   0.001070    red\n",
       "\n",
       "[486880 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_feature_rank_df = MacroDataRefinement.intent_keyword_feature_rankings(LR_intent_classifier_model, tfidf_vectorizer)\n",
    "intent_feature_rank_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will take the top 5 words for each intent (TODO: remove this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features_per_intent_df = intent_feature_rank_df.sort_values('coef', ascending=False).groupby('class').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features_per_intent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of those these are the ones that overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features_per_intent_df[top_5_features_per_intent_df.duplicated(\n",
    "    subset=['feature'], keep=False)].sort_values(['feature']).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_features_per_intent_df[top_5_features_per_intent_df['feature'].str.contains('remove')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro Intent Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what works and what doesn't, we can start refining the intents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want all of the columns, so we will drop some to review the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_scenario_df = nlu_data_df.drop(\n",
    "    columns=[\n",
    "        'userid', 'notes', 'answer', 'answerid', 'suggested_entities'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a domain (scenario) to review\n",
    "\n",
    "For this example we are going to pick `alarm` as an example but for actual refinement, pick whatever you like.\n",
    "\n",
    "The intent classification isn't bad, but the entity extraction for alarm_type is terrible. Perhaps it overlaps with another entity type, like 'event_name'. We will try to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add in the lists of domains and select one (use input)\n",
    "\n",
    "domain = 'alarm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>classifier</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alarm</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.899174</td>\n",
       "      <td>0.942808</td>\n",
       "      <td>605.0</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  domain  precision    recall  f1-score  support          classifier encoding\n",
       "0  alarm   0.990893  0.899174  0.942808    605.0  LogisticRegression    tfidf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_report_df[domain_report_df['domain'] == domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>status</th>\n",
       "      <th>answer_annotation</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : five am] [date : this week]</td>\n",
       "      <td>wake me up at five am this week</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : nine am] on [date : friday]</td>\n",
       "      <td>wake me up at nine am on friday</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set an alarm for [time : two hours from now]</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cancel my [time : seven am] alarm</td>\n",
       "      <td>cancel my seven am alarm</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remove the alarm set for [time : ten pm]</td>\n",
       "      <td>remove the alarm set for ten pm</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10721</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alarm [time : five pm] [date : tuesday]</td>\n",
       "      <td>alarm five pm tuesday</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set my calendar for an alarm at [time : five p...</td>\n",
       "      <td>set my calendar for an alarm at five pm tuesday</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10727</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set an alarm for [time : four pm] [date : tues...</td>\n",
       "      <td>set an alarm for four pm tuesday</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10818</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set alarm for this event reminder [event_name ...</td>\n",
       "      <td>set alarm for this event reminder repeating fo...</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17347</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when was the last alarm set</td>\n",
       "      <td>when was the last alarm set</td>\n",
       "      <td>How would you ask your PDA a general knowledge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scenario        intent status  \\\n",
       "0        alarm     alarm_set    NaN   \n",
       "1        alarm     alarm_set    NaN   \n",
       "2        alarm     alarm_set    NaN   \n",
       "42       alarm  alarm_remove    NaN   \n",
       "43       alarm  alarm_remove    NaN   \n",
       "...        ...           ...    ...   \n",
       "10721    alarm     alarm_set    NaN   \n",
       "10722    alarm     alarm_set    NaN   \n",
       "10727    alarm     alarm_set    NaN   \n",
       "10818    alarm     alarm_set    NaN   \n",
       "17347    alarm   alarm_query    NaN   \n",
       "\n",
       "                                       answer_annotation  \\\n",
       "0      wake me up at [time : five am] [date : this week]   \n",
       "1      wake me up at [time : nine am] on [date : friday]   \n",
       "2           set an alarm for [time : two hours from now]   \n",
       "42                     cancel my [time : seven am] alarm   \n",
       "43              remove the alarm set for [time : ten pm]   \n",
       "...                                                  ...   \n",
       "10721            alarm [time : five pm] [date : tuesday]   \n",
       "10722  set my calendar for an alarm at [time : five p...   \n",
       "10727  set an alarm for [time : four pm] [date : tues...   \n",
       "10818  set alarm for this event reminder [event_name ...   \n",
       "17347                        when was the last alarm set   \n",
       "\n",
       "                                       answer_normalised  \\\n",
       "0                        wake me up at five am this week   \n",
       "1                        wake me up at nine am on friday   \n",
       "2                    set an alarm for two hours from now   \n",
       "42                              cancel my seven am alarm   \n",
       "43                       remove the alarm set for ten pm   \n",
       "...                                                  ...   \n",
       "10721                              alarm five pm tuesday   \n",
       "10722    set my calendar for an alarm at five pm tuesday   \n",
       "10727                   set an alarm for four pm tuesday   \n",
       "10818  set alarm for this event reminder repeating fo...   \n",
       "17347                        when was the last alarm set   \n",
       "\n",
       "                                                question  \n",
       "0      Write what you would tell your PDA in the foll...  \n",
       "1      Write what you would tell your PDA in the foll...  \n",
       "2      Write what you would tell your PDA in the foll...  \n",
       "42     Write what you would tell your PDA in the foll...  \n",
       "43     Write what you would tell your PDA in the foll...  \n",
       "...                                                  ...  \n",
       "10721  Write what you would tell your PDA in the foll...  \n",
       "10722  Write what you would tell your PDA in the foll...  \n",
       "10727  Write what you would tell your PDA in the foll...  \n",
       "10818  Write what you would tell your PDA in the foll...  \n",
       "17347  How would you ask your PDA a general knowledge...  \n",
       "\n",
       "[605 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu_scenario_df = nlu_scenario_df[\n",
    "    nlu_scenario_df['scenario'] == domain\n",
    "]\n",
    "nlu_scenario_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add in description of the types of fixes we can do to the NLU data for intent\n",
    "* intents that collide with other intents and how to fix them (separation by TFIDF terms and using checkboxes in ipysheet to annotate them into the correct intent): this leads to the visualization of the intents in the NLU data with venn word cloud diagrams\n",
    "* utterances that are grammatically incorrect or contain incorrect spelling (grammar checker in the future?)\n",
    "* utterances that are straight up wrong for the intent\n",
    "* utterances that actually seem contain multiple intents (this isn't supported by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train an intent classifier on the whole data set for labeling intents and get the incorrect results for the intents on the domain we want to clean.\n",
    "(why not split a training test set? Because we want to see the results of the intent classifier on the whole data set, I mean if it's still getting it wrong when it has trained on it, then perhaps there is something wrong with the utterance, tagging, overlapping intents, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for every intent in the intent column, get the top 5 tfidf features and their scores\n",
    "# Like this: https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "#TODO: Make sure to pass them to the intent refinement process for each intent by putting them in the report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>status</th>\n",
       "      <th>answer_annotation</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>question</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stop [time : seven am] alarm</td>\n",
       "      <td>stop seven am alarm</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>alarm_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_remove</td>\n",
       "      <td>NaN</td>\n",
       "      <td>delete the alarm i just set</td>\n",
       "      <td>delete the alarm i just set</td>\n",
       "      <td>How would you ask your PDA to remove an alarm ...</td>\n",
       "      <td>alarm_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alert me at [time : three pm] to goto the [eve...</td>\n",
       "      <td>alert me at three pm to goto the concert</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>calendar_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>did i set an alarm to [alarm_type : wake up] i...</td>\n",
       "      <td>did i set an alarm to wake up in the morning</td>\n",
       "      <td>How would you ask your PDA to tell you about t...</td>\n",
       "      <td>alarm_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>do i have an alarm set</td>\n",
       "      <td>do i have an alarm set</td>\n",
       "      <td>How would you ask your PDA to tell you about t...</td>\n",
       "      <td>alarm_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set a repeating reminder alarm for the [media_...</td>\n",
       "      <td>set a repeating reminder alarm for the faceboo...</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>calendar_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10270</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set an alert for [time : two pm]</td>\n",
       "      <td>set an alert for two pm</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>calendar_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10469</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>schedule an alarm for [date : next weeks] [eve...</td>\n",
       "      <td>schedule an alarm for next weeks dentist appoi...</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>calendar_query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10818</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set alarm for this event reminder [event_name ...</td>\n",
       "      <td>set alarm for this event reminder repeating fo...</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>calendar_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17347</th>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>when was the last alarm set</td>\n",
       "      <td>when was the last alarm set</td>\n",
       "      <td>How would you ask your PDA a general knowledge...</td>\n",
       "      <td>alarm_set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scenario        intent status  \\\n",
       "44       alarm  alarm_remove    NaN   \n",
       "179      alarm  alarm_remove    NaN   \n",
       "210      alarm     alarm_set    NaN   \n",
       "212      alarm   alarm_query    NaN   \n",
       "512      alarm   alarm_query    NaN   \n",
       "...        ...           ...    ...   \n",
       "10258    alarm     alarm_set    NaN   \n",
       "10270    alarm     alarm_set    NaN   \n",
       "10469    alarm     alarm_set    NaN   \n",
       "10818    alarm     alarm_set    NaN   \n",
       "17347    alarm   alarm_query    NaN   \n",
       "\n",
       "                                       answer_annotation  \\\n",
       "44                          stop [time : seven am] alarm   \n",
       "179                          delete the alarm i just set   \n",
       "210    alert me at [time : three pm] to goto the [eve...   \n",
       "212    did i set an alarm to [alarm_type : wake up] i...   \n",
       "512                               do i have an alarm set   \n",
       "...                                                  ...   \n",
       "10258  set a repeating reminder alarm for the [media_...   \n",
       "10270                   set an alert for [time : two pm]   \n",
       "10469  schedule an alarm for [date : next weeks] [eve...   \n",
       "10818  set alarm for this event reminder [event_name ...   \n",
       "17347                        when was the last alarm set   \n",
       "\n",
       "                                       answer_normalised  \\\n",
       "44                                   stop seven am alarm   \n",
       "179                          delete the alarm i just set   \n",
       "210             alert me at three pm to goto the concert   \n",
       "212         did i set an alarm to wake up in the morning   \n",
       "512                               do i have an alarm set   \n",
       "...                                                  ...   \n",
       "10258  set a repeating reminder alarm for the faceboo...   \n",
       "10270                            set an alert for two pm   \n",
       "10469  schedule an alarm for next weeks dentist appoi...   \n",
       "10818  set alarm for this event reminder repeating fo...   \n",
       "17347                        when was the last alarm set   \n",
       "\n",
       "                                                question predicted_label  \n",
       "44     Write what you would tell your PDA in the foll...       alarm_set  \n",
       "179    How would you ask your PDA to remove an alarm ...       alarm_set  \n",
       "210    Write what you would tell your PDA in the foll...    calendar_set  \n",
       "212    How would you ask your PDA to tell you about t...       alarm_set  \n",
       "512    How would you ask your PDA to tell you about t...       alarm_set  \n",
       "...                                                  ...             ...  \n",
       "10258  Write what you would tell your PDA in the foll...    calendar_set  \n",
       "10270  Write what you would tell your PDA in the foll...    calendar_set  \n",
       "10469  Write what you would tell your PDA in the foll...  calendar_query  \n",
       "10818  Write what you would tell your PDA in the foll...    calendar_set  \n",
       "17347  How would you ask your PDA a general knowledge...       alarm_set  \n",
       "\n",
       "[111 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_intent_predictions_df = IntentMatcher.get_incorrect_predicted_labels(\n",
    "    nlu_scenario_df, LR_intent_classifier_model, tfidf_vectorizer)\n",
    "incorrect_intent_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent: alarm_remove\n",
      " f1 score: 0.67\n",
      " total count: 117, total incorrect count: 36\n",
      " example of correctly predicted utterance: cancel my seven am alarm\n",
      " example of incorrectly predicted utterance: stop [time : seven am] alarm\n",
      "\n",
      "incorrect predicted intents for alarm_remove and their counts:\n",
      "alarm_set          19\n",
      "alarm_query        10\n",
      "quirky              3\n",
      "commandstop         2\n",
      "calendar_remove     1\n",
      "lists_remove        1\n",
      "\n",
      "intent: alarm_set\n",
      " f1 score: 0.79\n",
      " total count: 289, total incorrect count: 40\n",
      " example of correctly predicted utterance: wake me up at five am this week\n",
      " example of incorrectly predicted utterance: alert me at [time : three pm] to goto the [event_name : concert]\n",
      "\n",
      "incorrect predicted intents for alarm_set and their counts:\n",
      "calendar_set      28\n",
      "alarm_remove       3\n",
      "datetime_query     2\n",
      "alarm_query        2\n",
      "calendar_query     2\n",
      "sendemail          1\n",
      "quirky             1\n",
      "game               1\n",
      "\n",
      "intent: alarm_query\n",
      " f1 score: 0.79\n",
      " total count: 199, total incorrect count: 35\n",
      " example of correctly predicted utterance: what alarms i have set\n",
      " example of incorrectly predicted utterance: did i set an alarm to [alarm_type : wake up] in the [timeofday : morning]\n",
      "\n",
      "incorrect predicted intents for alarm_query and their counts:\n",
      "alarm_set         22\n",
      "calendar_query     3\n",
      "quirky             3\n",
      "calendar_set       2\n",
      "datetime_query     2\n",
      "takeaway_query     1\n",
      "lists_query        1\n",
      "email_query        1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incorrect_predicted_intents_report = MacroDataRefinement.get_incorrect_predicted_intents_report(\n",
    "    nlu_scenario_df, incorrect_intent_predictions_df, intent_report_df, intent_feature_rank_df)\n",
    "\n",
    "#TODO: export to an md file (and format it!), or better yet: just make it a JSON file!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alarm_remove': {'f1_score': 0.67,\n",
       "  'total_count': 117,\n",
       "  'total_incorrect_count': 36,\n",
       "  'top_features': ['alarm',\n",
       "   'remove',\n",
       "   'alarms',\n",
       "   'cancel',\n",
       "   'off',\n",
       "   'removed',\n",
       "   'disable',\n",
       "   'delete',\n",
       "   'wake',\n",
       "   'rid'],\n",
       "  'correct_utterance_example': ['alarm_remove',\n",
       "   'cancel my [time : seven am] alarm',\n",
       "   [('cancel', 3.2256674655709157),\n",
       "    ('my', 0.8476517195850537),\n",
       "    ('seven', 0.48199028886100836),\n",
       "    ('am', 1.548025808183762),\n",
       "    ('alarm', 7.808361134747968)]],\n",
       "  'incorrect_utterance_example': ['alarm_set',\n",
       "   'stop [time : seven am] alarm',\n",
       "   [('stop', 0.6226635805696362),\n",
       "    ('seven', 0.48199028886100836),\n",
       "    ('am', 1.548025808183762),\n",
       "    ('alarm', 7.808361134747968)]],\n",
       "  'incorrect_predicted_intents_and_counts': {'alarm_set': 19,\n",
       "   'alarm_query': 10,\n",
       "   'quirky': 3,\n",
       "   'commandstop': 2,\n",
       "   'calendar_remove': 1,\n",
       "   'lists_remove': 1}},\n",
       " 'alarm_set': {'f1_score': 0.79,\n",
       "  'total_count': 289,\n",
       "  'total_incorrect_count': 40,\n",
       "  'top_features': ['alarm',\n",
       "   'wake',\n",
       "   'set',\n",
       "   'at',\n",
       "   'an',\n",
       "   'up',\n",
       "   'am',\n",
       "   'five',\n",
       "   'clock',\n",
       "   'alert'],\n",
       "  'correct_utterance_example': ['alarm_set',\n",
       "   'wake me up at [time : five am] [date : this week]',\n",
       "   [('wake', 4.429454739550537),\n",
       "    ('me', 1.6784361992946737),\n",
       "    ('up', 3.10965451805076),\n",
       "    ('at', 3.966924030652687),\n",
       "    ('five', 2.4678333886706603),\n",
       "    ('am', 3.1009773743019937),\n",
       "    ('this', -0.1232999836618595),\n",
       "    ('week', 0.004163977307692039)]],\n",
       "  'incorrect_utterance_example': ['calendar_set',\n",
       "   'alert me at [time : three pm] to goto the [event_name : concert]',\n",
       "   [('alert', 2.17627633681955),\n",
       "    ('me', 1.6784361992946737),\n",
       "    ('at', 3.966924030652687),\n",
       "    ('three', 0.9769636618041411),\n",
       "    ('pm', 1.889988251259949),\n",
       "    ('to', -0.06723131171521263),\n",
       "    ('goto', 0.5480203563956415),\n",
       "    ('the', -1.3741962191451664),\n",
       "    ('concert', 0.3558011676073562)]],\n",
       "  'incorrect_predicted_intents_and_counts': {'calendar_set': 28,\n",
       "   'alarm_remove': 3,\n",
       "   'datetime_query': 2,\n",
       "   'alarm_query': 2,\n",
       "   'calendar_query': 2,\n",
       "   'sendemail': 1,\n",
       "   'quirky': 1,\n",
       "   'game': 1}},\n",
       " 'alarm_query': {'f1_score': 0.79,\n",
       "  'total_count': 199,\n",
       "  'total_incorrect_count': 35,\n",
       "  'top_features': ['alarms',\n",
       "   'alarm',\n",
       "   'set',\n",
       "   'have',\n",
       "   'morning',\n",
       "   'there',\n",
       "   'are',\n",
       "   'time',\n",
       "   'what',\n",
       "   'wake'],\n",
       "  'correct_utterance_example': ['alarm_query',\n",
       "   'what alarms i have set',\n",
       "   [('what', 1.6945524875676055),\n",
       "    ('alarms', 10.160949839305223),\n",
       "    ('i', 0),\n",
       "    ('have', 2.7756179198184143),\n",
       "    ('set', 3.4905232830032316)]],\n",
       "  'incorrect_utterance_example': ['alarm_set',\n",
       "   'did i set an alarm to [alarm_type : wake up] in the [timeofday : morning]',\n",
       "   [('did', 1.0675053785146678),\n",
       "    ('i', 0),\n",
       "    ('set', 3.4905232830032316),\n",
       "    ('an', -0.28989547438284924),\n",
       "    ('alarm', 7.404020879827028),\n",
       "    ('to', -0.34517875050179136),\n",
       "    ('wake', 1.6935175471439943),\n",
       "    ('up', 1.131419716777588),\n",
       "    ('in', -0.4808495157214086),\n",
       "    ('the', 0.7043993027456317),\n",
       "    ('morning', 2.3474463997986037)]],\n",
       "  'incorrect_predicted_intents_and_counts': {'alarm_set': 22,\n",
       "   'calendar_query': 3,\n",
       "   'quirky': 3,\n",
       "   'calendar_set': 2,\n",
       "   'datetime_query': 2,\n",
       "   'takeaway_query': 1,\n",
       "   'lists_query': 1,\n",
       "   'email_query': 1}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_predicted_intents_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This does not yield helpful information! I will probably get rid of this\n",
    "nlu_scenario_correct_df = nlu_scenario_df[~nlu_scenario_df.index.isin(\n",
    "    incorrect_intent_predictions_df.index)]\n",
    "\n",
    "correct_intent_term_counts = nlu_scenario_correct_df[nlu_scenario_correct_df['intent']\n",
    "                                                     == 'alarm_remove']['answer_normalised'].str.split(' ').explode().value_counts()\n",
    "\n",
    "correct_intent_term_counts\n",
    "\n",
    "incorrect_intent_term_counts = incorrect_intent_predictions_df[incorrect_intent_predictions_df[\"intent\"]\n",
    "                                                               == 'alarm_remove']['answer_normalised'].str.split(' ').explode().value_counts()\n",
    "\n",
    "incorrect_intent_term_counts[~incorrect_intent_term_counts.index.isin(correct_intent_term_counts.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: experiement with the term frequencies of the incorrect predictions for the intent and the correct predictions for the incorrect predicted labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_feature_rank_df[(intent_feature_rank_df['class'] == 'alarm_remove') and (intent_feature_rank_df['feature'] == 'alarm')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_feature_rank_df[intent_feature_rank_df['class']\n",
    "                       == 'alarm_remove'].sort_values('coef', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_feature_rank_df[intent_feature_rank_df['class'] == 'alarm_set'].sort_values('coef', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_feature_rank_df[intent_feature_rank_df['class']\n",
    "                       == 'alarm_query'].sort_values('coef', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted_intents_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make a report similar to the intent keyword report, but for the incorrect predicted intents.\n",
    "# What keywords are sending these utterances to the wrong intent? (do all utterances for an intent that have that keyword get sent to the wrong intent?!)\n",
    "# How do you interpret the results? Is it because of an intent overlap, utterances that are incorrectly written or labeled, or is the basic NLU engine breaking down?\n",
    "# How do you seperate the different types of errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: get the features from each intent that fails to match the intent (how?)\n",
    "\n",
    "#TODO: possible solutions: get top features for each incorrect predicted intents and the intents they are being incorrectly predicted as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_intent_predictions_df\n",
    "#1. TODO: get features for intent and predicted intent from intent_feature_rank_df\n",
    "    #TODO: get list of intents and predicted_label from incorrect_intent_predictions_df\n",
    "    #TODO: get all features for each intent and predicted_label from intent_feature_rank_df\n",
    "    #TODO: find where the features overlap for each intent and the incorrectly predicted label\n",
    "    #TODO: find examples of utterances that have these features for each intent, both correctly and incorrectly predicted \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: find separation criteria for alarm_set and calander_set: get most popular tfidf words (and/or coef features) for each intent and assign them as the separation criteria. e.g.\n",
    "# alarm -> alarm_set\n",
    "# reminder -> calander_set\n",
    "# is alert alarm or reminder?\n",
    "# wakeup or wake up -> alarm_set\n",
    "# get up -> alarm_set\n",
    "# timer -> remove, it's not a timer!\n",
    "\n",
    "#TODO: this will be later visually represented in a venn diagram with word clouds and forms the basis for refinement besides the sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: human in the for loop.\n",
    "\n",
    "You have made it this far, now it's your turn to shine human!\n",
    "\n",
    "You will provide a refinement to each incorrectly predicted intent. some of the incorrectly predicted utterances are actually fine the way they are, you may need to review the intent that is fasly being predicted...\n",
    "\n",
    "\n",
    "Besides correcting the utterances(ie spelling), you can also mark an entry with the following:\n",
    "* **review**: the utterance needs to be reviewed again by a human\n",
    "* **move**: the utterance needs to be moved to another intent(NOTE: if you have a big data set, it might be better to just **remove** the utterance from the data set)\n",
    "* **remove**: the utterance should be removed from the dataset\n",
    "\n",
    "You can use your human ability to refine the NLU intent data by answering the following questions:\n",
    "\n",
    "1. Does the utterance fit to the intent? -> mark as move, remove, or review\n",
    "\n",
    "2. Is the utterance grammar or spelling wrong but(1) is fine? -> correct the utterance\n",
    "\n",
    "3. Is this intent collidating with another intent because the scope of both intents are overlapping? -> redefine the scope of the intents(either combine them or separate their functionality better)\n",
    "\n",
    "4. Is the intent collidating with another intent because certain keywords overlap between intents? -> redefine the keywords to split between intents or merge them together if they are similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from here it's all just a work in progress. These 4 flows should be implemented in a human for loop pipeline with ipysheets.\n",
    "# TODO: at the end of each flow, the dataframe will be appended with a column to indicate MDR was successfully completed. This way users can keep track of what they have refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_refinement_dictionary = MacroDataRefinement.get_intent_dataframes_to_refine(incorrect_intent_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above intents, pick one to refine. It's probably a good idea to look at the `MacroDataRefinement.get_incorrect_predicted_intents_report` and start with the one with the most incorrect predictions.\n",
    "\n",
    "Put that intent into the quotes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change to input field\n",
    "to_review_sheet = MacroDataRefinement.create_sheet(intent_refinement_dictionary['alarm_query'])\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a really good idea to convert the sheet back to a dataframe and convert it to a csv to be saved.\n",
    "\n",
    "NOTE: make sure to replace the `domain` with the domain and `intent` in the csv file name to the actual domain and intent name you have refined in both `to_csv()` and `load_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = MacroDataRefinement.convert_sheet_to_dataframe(to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df.to_csv(\n",
    "    'data/reviewed/reviewed_alarm_alarm_query_incorrectly_predicted_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = pd.read_csv(\n",
    "    'data/reviewed/reviewed_alarm_alarm_query_incorrectly_predicted_df.csv', sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move remove_entries_marked_remove to the end of the flow (so that when joining together back to the main dataset, those entries are removed there too!)\n",
    "#TODO: add in list of intents above this cell so people can see if it's the right intent to pick\n",
    "\n",
    "refined_intent_df = reviewed_intent_df.apply(\n",
    "    MacroDataRefinement.move_entry, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df.to_csv(\n",
    "    'data/refined/refined_alarm_alarm_query_incorrectly_predicted_df.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = pd.read_csv(\n",
    "    'data/refined/refined_alarm_alarm_query_incorrectly_predicted_df.csv', sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this can be removed in the future, I forgot to remove these entries when I did my own refinement so I drop them for now\n",
    "refined_intent_df = refined_intent_df[~refined_intent_df['status'].str.contains(\n",
    "    'IRR', na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move this and the next cell into a function in the macro data refinement class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = refined_intent_df.assign(intent_refined=True)\n",
    "refined_intent_df['intent_refined'] = refined_intent_df['intent_refined'].astype(bool)\n",
    "refined_intent_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_refined_df = nlu_data_df.merge(refined_intent_df, how='left', left_index=True, right_index=True)\n",
    "nlu_data_refined_df['scenario_y'].fillna(nlu_data_refined_df['scenario_x'], inplace=True)\n",
    "nlu_data_refined_df['intent_y'].fillna(nlu_data_refined_df['intent_x'], inplace=True)\n",
    "nlu_data_refined_df['answer_annotation_y'].fillna(nlu_data_refined_df['answer_annotation_x'], inplace=True)\n",
    "nlu_data_refined_df['status_y'].fillna(nlu_data_refined_df['status_x'], inplace=True)\n",
    "nlu_data_refined_df['intent_refined'].fillna(\n",
    "    False, inplace=True)\n",
    "nlu_data_refined_df.drop(columns=['scenario_x', 'intent_x', 'answer_annotation_x', 'status_x', 'move'], inplace=True)\n",
    "nlu_data_refined_df.rename(columns={'scenario_y': 'scenario', 'intent_y': 'intent', 'answer_annotation_y': 'answer_annotation', 'status_y': 'status'}, inplace=True)\n",
    "nlu_data_refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_refined_df[nlu_data_refined_df['remove'] != True]\n",
    "\n",
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=removed_nlu_data_refined_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "\n",
    "refined_incorrect_intent_predictions_df = IntentMatcher.get_incorrect_predicted_labels(\n",
    "    removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == 'alarm'], LR_intent_classifier_model, tfidf_vectorizer)\n",
    "refined_incorrect_intent_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_intent_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "improved_intent_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MacroDataRefinement.get_incorrect_predicted_intents_report(\n",
    "    removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == 'alarm'], refined_incorrect_intent_predictions_df, improved_intent_report_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides some incorrect utterances and intents, we can see that there is an overlap between the intent 'alarm_set' and the intent 'calandar_set'. This is because those two intents are not well defined and will require refinement. We will try to fix this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: integrate refined_intent_df into the main dataset and save it as nlu_data_refined_df\n",
    "\n",
    "\n",
    "#TODO: export nlu_data_refined_df to a csv file and save it as NLU-Data-Home-Domain-Annotated-Refined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for every intent in the predicted intent column, get the top 5 tfidf features and their scores\n",
    "# Like this: https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "#TODO: Make sure to pass them to the intent refinement process for each intent by putting them in the report!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: get the counts of the terms from the utterances that are incorrect for a specific domain (should I filter by tfidf scores?)\n",
    "#TODO: Look up the most popular terms for an intent if they are red or green for that intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word (feature) in the utterances, we get the coeficients for the intents.\n",
    "# From the shape, we see it contains the classes and the coeficients.\n",
    "coefs = LR_intent_classifier_model.coef_\n",
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlu_engine import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the encoded classes\n",
    "classes = LR_intent_classifier_model.classes_\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cant to get the actual feature names (the words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try an example with TFIDF only, this only tells us overall the TFIDF score for each word, not related to the intent\n",
    "from utils.nlu_engine import TfidfEncoder\n",
    "\n",
    "utterance = 'turn off the alarm I set'\n",
    "\n",
    "response = TfidfEncoder.encode_vectors(\n",
    "    utterance, tfidf_vectorizer)\n",
    "\n",
    "for vector in response.nonzero()[1]:\n",
    "    print(f'word: {feature_names[vector]} - ranking: {response[0, vector]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rip out a list of tuples for the features and their coeficients for the intent\n",
    "output = []\n",
    "for classIndex, features in enumerate(coefs):\n",
    "    for featureIndex, feature in enumerate(features):\n",
    "        output.append(\n",
    "            (classes[classIndex], feature_names[featureIndex], feature))\n",
    "feature_rank_df = pd.DataFrame(output, columns=['class', 'feature', 'coef'])\n",
    "feature_rank_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a good idea to convert the classes from the encoded to a normal human form\n",
    "feature_rank_df['class'] = LabelEncoder.inverse_transform(feature_rank_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the features by the absolute value of their coefficient and color them red or green\n",
    "feature_rank_df[\"abs_value\"] = feature_rank_df[\"coef\"].apply(lambda x: abs(x))\n",
    "feature_rank_df[\"colors\"] = feature_rank_df[\"coef\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "feature_rank_df = feature_rank_df.sort_values(\"abs_value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at an example of the word 'set'\n",
    "feature_rank_df[(feature_rank_df['feature'] == 'set') & (feature_rank_df['colors'] == 'red')\n",
    "   ].sort_values('abs_value', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity extraction report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement brown clustering to improve entity extraction (see entity_extractor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(data_df=nlu_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Benchmark the state features to find the best and the worst, remove/replace worst: add in state features like here: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
    "# Specifically, we want print_state_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from the entity extraction report, the entity extraction is not working for the alarm_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: move this below the intent cleaning flow\n",
    "nlu_scenario_df = nlu_scenario_df[nlu_scenario_df['answer_annotation'].str.contains(\n",
    "    'alarm_type')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Convert to ipysheet and review\n",
    "TODO: add in description of the types of fixes we can do to the NLU data for entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: same as above for intents but with predicted entities: report on them, break them down into a dictionary of dataframes and refine them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with 'alarm' and the alarm_type:\n",
    "* We see that the alarm_type entities are really event_name(ie wake up, soccer practice) except for ID 5879, we will need to change them to event_name and remove ID 5879.\n",
    "* The last one(ID 6320) is a mistake. Someone got confused with the prompt and assumed alarm is a security system. This is out of scope for the alarm domain, as the alarms are ones set on a phone or other device. We will drop this utterance.\n",
    "Once you are done reviewing, you convert it back to a dataframe and check to make sure it looks okay.\n",
    "Let's change all alarm_type entities to event_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewed_scenario_df['answer_annotation'] = reviewed_scenario_df['answer_annotation'].str.replace(\n",
    "    'alarm_type', 'event_name')\n",
    "reviewed_scenario_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay dokey, now we can merge this with the original data set and see if it made a difference already(well of course it did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df.drop(\n",
    "    reviewed_scenario_df[reviewed_scenario_df['remove'] == True].index, inplace=True)\n",
    "\n",
    "reviewed_scenario_df = reviewed_scenario_df[~reviewed_scenario_df['remove'] == True]\n",
    "\n",
    "nlu_data_df.loc[nlu_data_df.index.intersection(\n",
    "    reviewed_scenario_df.index), 'answer_annotation'] = reviewed_scenario_df['answer_annotation']\n",
    "\n",
    "nlu_data_df[(nlu_data_df['scenario'].str.contains('alarm')) & (nlu_data_df['answer_annotation'].str.contains(\n",
    "    'event_name'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark changed data set\n",
    "TODO: repeat reports for the changed data set for domain and entities and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_reviewed_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=nlu_data_df)\n",
    "entity_reviewed_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure it is okay, you can save it as a csv file, make sure to name it correctly(i.e. `alarm_domain_first_review.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_scenario_df.to_csv('alarm_domain_first_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back up and check to make sure it looks okay. Make sure to give it the right name!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_domain_first_review_df = pd.read_csv(\n",
    "    'alarm_domain_first_review.csv', index_col=0)\n",
    "audio_domain_first_review_df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the evaluate_classifier in the NLU engine to check f1 score for intents and entities in the domain vs original NLU data of domain!\n",
    "# Value: benchmark!\n",
    "#TODO: implement a flow for getting the domains with the lowest f1 scores by intent/domain and entities and cleaning them by the order of the lowest f1 scores\n",
    "# TODO: concat all reviewed dfs and save to csv\n",
    "# TODO: add benchmark for whole NLU data set before and after cleaning! (by intents and domains!)\n",
    "# TODO: review the review marked entries\n",
    "# TODO: add new column for notes\n",
    "# TODO: change flow of review for only ones that should be reviewed, not all of the ones that have been changed (track changes by comparing against the original data set)\n",
    "# TODO: do the changed utterances have to be changed in other fields too or is it just enough for the tagged utterancve field?\n",
    "# TODO: add visualizations of domains, their intents, keywords in utterances, and entities to top\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

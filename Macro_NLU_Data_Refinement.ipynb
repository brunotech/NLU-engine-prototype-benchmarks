{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipysheet\n",
    "import nltk\n",
    "\n",
    "from utils.nlu_engine import NLUEngine\n",
    "from utils.nlu_engine import MacroDataRefinement\n",
    "from utils.nlu_engine import DataUtils\n",
    "from utils.nlu_engine import IntentMatcher, LR\n",
    "from utils.nlu_engine import EntityExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, youâ€™ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Refactor with text inputs for increased flexibility like this: https://queirozf.com/entries/interactive-controls-for-jupyter-notebooks-python-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = DataUtils.load_data(\n",
    "    'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intent classifier report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a report by domain classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_labels = 'scenario'\n",
    "\n",
    "domain_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict=domain_labels,\n",
    "    classifier=LR\n",
    ")\n",
    "\n",
    "domain_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's do a report by intent classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "intent_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro Intent Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what works and what doesn't, we can start refining the intents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want all of the columns, so we will drop some to review the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_scenario_df = nlu_data_df.drop(\n",
    "    columns=[\n",
    "        'userid', 'notes', 'answer', 'answerid', 'suggested_entities'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a domain (scenario) to review\n",
    "\n",
    "For this example we are going to pick `alarm` as an example but for actual refinement, pick whatever you like.\n",
    "\n",
    "The intent classification isn't bad, but the entity extraction for alarm_type is terrible. Perhaps it overlaps with another entity type, like 'event_name'. We will try to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add in the lists of domains and select one (use input)\n",
    "\n",
    "domain = 'alarm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_report_df[domain_report_df['domain'] == domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_scenario_df = nlu_scenario_df[\n",
    "    nlu_scenario_df['scenario'] == domain\n",
    "]\n",
    "nlu_scenario_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add in description of the types of fixes we can do to the NLU data for intent\n",
    "* intents that collide with other intents and how to fix them (separation by TFIDF terms and using checkboxes in ipysheet to annotate them into the correct intent): this leads to the visualization of the intents in the NLU data with venn word cloud diagrams\n",
    "* utterances that are grammatically incorrect or contain incorrect spelling (grammar checker in the future?)\n",
    "* utterances that are straight up wrong for the intent\n",
    "* utterances that actually seem contain multiple intents (this isn't supported by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train an intent classifier on the whole data set for labeling intents and get the incorrect results for the intents on the domain we want to clean.\n",
    "(why not split a training test set? Because we want to see the results of the intent classifier on the whole data set, I mean if it's still getting it wrong when it has trained on it, then perhaps there is something wrong with the utterance, tagging, overlapping intents, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for every intent in the intent column, get the top 5 tfidf features and their scores\n",
    "# Like this: https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "#TODO: Make sure to pass them to the intent refinement process for each intent by putting them in the report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_intent_predictions_df = IntentMatcher.get_incorrect_predicted_labels(\n",
    "    nlu_scenario_df, LR_intent_classifier_model, tfidf_vectorizer)\n",
    "incorrect_intent_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MacroDataRefinement.get_incorrect_predicted_intents_report(\n",
    "    nlu_scenario_df, incorrect_intent_predictions_df, intent_report_df)\n",
    "\n",
    "#TODO: export to an md file (and format it!), or better yet: just make it a JSON file!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: find separation criteria for alarm_set and calander_set: get most popular tfidf words (and/or coef features) for each intent and assign them as the separation criteria. e.g.\n",
    "# alarm -> alarm_set\n",
    "# reminder -> calander_set\n",
    "# is alert alarm or reminder?\n",
    "# wakeup or wake up -> alarm_set\n",
    "# get up -> alarm_set\n",
    "# timer -> remove, it's not a timer!\n",
    "\n",
    "#TODO: this will be later visually represented in a venn diagram with word clouds and forms the basis for refinement besides the sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: human in the for loop.\n",
    "\n",
    "You have made it this far, now it's your turn to shine human!\n",
    "\n",
    "You will provide a refinement to each incorrectly predicted intent. some of the incorrectly predicted utterances are actually fine the way they are, you may need to review the intent that is fasly being predicted...\n",
    "\n",
    "\n",
    "Besides correcting the utterances(ie spelling), you can also mark an entry with the following:\n",
    "* **review**: the utterance needs to be reviewed again by a human\n",
    "* **move**: the utterance needs to be moved to another intent(NOTE: if you have a big data set, it might be better to just **remove** the utterance from the data set)\n",
    "* **remove**: the utterance should be removed from the dataset\n",
    "\n",
    "You can use your human ability to refine the NLU intent data by answering the following questions:\n",
    "\n",
    "1. Does the utterance fit to the intent? -> mark as move, remove, or review\n",
    "\n",
    "2. Is the utterance grammar or spelling wrong but(1) is fine? -> correct the utterance\n",
    "\n",
    "3. Is this intent collidating with another intent because the scope of both intents are overlapping? -> redefine the scope of the intents(either combine them or separate their functionality better)\n",
    "\n",
    "4. Is the intent collidating with another intent because certain keywords overlap between intents? -> redefine the keywords to split between intents or merge them together if they are similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: from here it's all just a work in progress. These 4 flows should be implemented in a human for loop pipeline with ipysheets.\n",
    "# TODO: at the end of each flow, the dataframe will be appended with a column to indicate MDR was successfully completed. This way users can keep track of what they have refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_refinement_dictionary = MacroDataRefinement.get_intent_dataframes_to_refine(incorrect_intent_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above intents, pick one to refine. It's probably a good idea to look at the `MacroDataRefinement.get_incorrect_predicted_intents_report` and start with the one with the most incorrect predictions.\n",
    "\n",
    "Put that intent into the quotes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change to input field\n",
    "to_review_sheet = MacroDataRefinement.create_sheet(intent_refinement_dictionary['alarm_query'])\n",
    "to_review_sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a really good idea to convert the sheet back to a dataframe and convert it to a csv to be saved.\n",
    "\n",
    "NOTE: make sure to replace the `domain` with the domain and `intent` in the csv file name to the actual domain and intent name you have refined in both `to_csv()` and `load_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = MacroDataRefinement.convert_sheet_to_dataframe(to_review_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df.to_csv(\n",
    "    'data/reviewed/reviewed_alarm_alarm_query_incorrectly_predicted_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_intent_df = pd.read_csv(\n",
    "    'data/reviewed/reviewed_alarm_alarm_query_incorrectly_predicted_df.csv', sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move remove_entries_marked_remove to the end of the flow (so that when joining together back to the main dataset, those entries are removed there too!)\n",
    "#TODO: add in list of intents above this cell so people can see if it's the right intent to pick\n",
    "\n",
    "refined_intent_df = reviewed_intent_df.apply(\n",
    "    MacroDataRefinement.move_entry, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df.to_csv(\n",
    "    'data/refined/refined_alarm_alarm_query_incorrectly_predicted_df.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = pd.read_csv(\n",
    "    'data/refined/refined_alarm_alarm_query_incorrectly_predicted_df.csv', sep=',', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: this can be removed in the future, I forgot to remove these entries when I did my own refinement so I drop them for now\n",
    "refined_intent_df = refined_intent_df[~refined_intent_df['status'].str.contains(\n",
    "    'IRR', na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: move this and the next cell into a function in the macro data refinement class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_intent_df = refined_intent_df.assign(intent_refined=True)\n",
    "refined_intent_df['intent_refined'] = refined_intent_df['intent_refined'].astype(bool)\n",
    "refined_intent_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_refined_df = nlu_data_df.merge(refined_intent_df, how='left', left_index=True, right_index=True)\n",
    "nlu_data_refined_df['scenario_y'].fillna(nlu_data_refined_df['scenario_x'], inplace=True)\n",
    "nlu_data_refined_df['intent_y'].fillna(nlu_data_refined_df['intent_x'], inplace=True)\n",
    "nlu_data_refined_df['answer_annotation_y'].fillna(nlu_data_refined_df['answer_annotation_x'], inplace=True)\n",
    "nlu_data_refined_df['status_y'].fillna(nlu_data_refined_df['status_x'], inplace=True)\n",
    "nlu_data_refined_df['intent_refined'].fillna(\n",
    "    False, inplace=True)\n",
    "nlu_data_refined_df.drop(columns=['scenario_x', 'intent_x', 'answer_annotation_x', 'status_x', 'move'], inplace=True)\n",
    "nlu_data_refined_df.rename(columns={'scenario_y': 'scenario', 'intent_y': 'intent', 'answer_annotation_y': 'answer_annotation', 'status_y': 'status'}, inplace=True)\n",
    "nlu_data_refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_refined_df[nlu_data_refined_df['remove'] != True]\n",
    "\n",
    "LR_intent_classifier_model, tfidf_vectorizer = NLUEngine.train_intent_classifier(\n",
    "    data_df_path=removed_nlu_data_refined_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "\n",
    "refined_incorrect_intent_predictions_df = IntentMatcher.get_incorrect_predicted_labels(\n",
    "    removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == 'alarm'], LR_intent_classifier_model, tfidf_vectorizer)\n",
    "refined_incorrect_intent_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_intent_report_df = NLUEngine.evaluate_intent_classifier(\n",
    "    data_df_path=nlu_data_df,\n",
    "    labels_to_predict='intent',\n",
    "    classifier=LR\n",
    ")\n",
    "improved_intent_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MacroDataRefinement.get_incorrect_predicted_intents_report(\n",
    "    removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == 'alarm'], refined_incorrect_intent_predictions_df, improved_intent_report_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides some incorrect utterances and intents, we can see that there is an overlap between the intent 'alarm_set' and the intent 'calandar_set'. This is because those two intents are not well defined and will require refinement. We will try to fix this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: integrate refined_intent_df into the main dataset and save it as nlu_data_refined_df\n",
    "\n",
    "\n",
    "#TODO: export nlu_data_refined_df to a csv file and save it as NLU-Data-Home-Domain-Annotated-Refined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for every intent in the predicted intent column, get the top 5 tfidf features and their scores\n",
    "# Like this: https://stackoverflow.com/questions/34232190/scikit-learn-tfidfvectorizer-how-to-get-top-n-terms-with-highest-tf-idf-score\n",
    "#TODO: Make sure to pass them to the intent refinement process for each intent by putting them in the report!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: get the counts of the terms from the utterances that are incorrect for a specific domain (should I filter by tfidf scores?)\n",
    "#TODO: Look up the most popular terms for an intent if they are red or green for that intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word (feature) in the utterances, we get the coeficients for the intents.\n",
    "# From the shape, we see it contains the classes and the coeficients.\n",
    "coefs = LR_intent_classifier_model.coef_\n",
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlu_engine import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the encoded classes\n",
    "classes = LR_intent_classifier_model.classes_\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cant to get the actual feature names (the words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try an example with TFIDF only, this only tells us overall the TFIDF score for each word, not related to the intent\n",
    "from utils.nlu_engine import TfidfEncoder\n",
    "\n",
    "utterance = 'turn off the alarm I set'\n",
    "\n",
    "response = TfidfEncoder.encode_vectors(\n",
    "    utterance, tfidf_vectorizer)\n",
    "\n",
    "for vector in response.nonzero()[1]:\n",
    "    print(f'word: {feature_names[vector]} - ranking: {response[0, vector]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rip out a list of tuples for the features and their coeficients for the intent\n",
    "output = []\n",
    "for classIndex, features in enumerate(coefs):\n",
    "    for featureIndex, feature in enumerate(features):\n",
    "        output.append(\n",
    "            (classes[classIndex], feature_names[featureIndex], feature))\n",
    "feature_rank_df = pd.DataFrame(output, columns=['class', 'feature', 'coef'])\n",
    "feature_rank_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a good idea to convert the classes from the encoded to a normal human form\n",
    "feature_rank_df['class'] = LabelEncoder.inverse_transform(feature_rank_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the features by the absolute value of their coefficient and color them red or green\n",
    "feature_rank_df[\"abs_value\"] = feature_rank_df[\"coef\"].apply(lambda x: abs(x))\n",
    "feature_rank_df[\"colors\"] = feature_rank_df[\"coef\"].apply(lambda x: \"green\" if x > 0 else \"red\")\n",
    "feature_rank_df = feature_rank_df.sort_values(\"abs_value\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at an example of the word 'set'\n",
    "feature_rank_df[(feature_rank_df['feature'] == 'set') & (feature_rank_df['colors'] == 'red')\n",
    "   ].sort_values('abs_value', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity extraction report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement brown clustering to improve entity extraction (see entity_extractor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(data_df=nlu_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Benchmark the state features to find the best and the worst, remove/replace worst: add in state features like here: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
    "# Specifically, we want print_state_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from the entity extraction report, the entity extraction is not working for the alarm_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: move this below the intent cleaning flow\n",
    "nlu_scenario_df = nlu_scenario_df[nlu_scenario_df['answer_annotation'].str.contains(\n",
    "    'alarm_type')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Convert to ipysheet and review\n",
    "TODO: add in description of the types of fixes we can do to the NLU data for entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: same as above for intents but with predicted entities: report on them, break them down into a dictionary of dataframes and refine them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with 'alarm' and the alarm_type:\n",
    "* We see that the alarm_type entities are really event_name(ie wake up, soccer practice) except for ID 5879, we will need to change them to event_name and remove ID 5879.\n",
    "* The last one(ID 6320) is a mistake. Someone got confused with the prompt and assumed alarm is a security system. This is out of scope for the alarm domain, as the alarms are ones set on a phone or other device. We will drop this utterance.\n",
    "Once you are done reviewing, you convert it back to a dataframe and check to make sure it looks okay.\n",
    "Let's change all alarm_type entities to event_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewed_scenario_df['answer_annotation'] = reviewed_scenario_df['answer_annotation'].str.replace(\n",
    "    'alarm_type', 'event_name')\n",
    "reviewed_scenario_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay dokey, now we can merge this with the original data set and see if it made a difference already(well of course it did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df.drop(\n",
    "    reviewed_scenario_df[reviewed_scenario_df['remove'] == True].index, inplace=True)\n",
    "\n",
    "reviewed_scenario_df = reviewed_scenario_df[~reviewed_scenario_df['remove'] == True]\n",
    "\n",
    "nlu_data_df.loc[nlu_data_df.index.intersection(\n",
    "    reviewed_scenario_df.index), 'answer_annotation'] = reviewed_scenario_df['answer_annotation']\n",
    "\n",
    "nlu_data_df[(nlu_data_df['scenario'].str.contains('alarm')) & (nlu_data_df['answer_annotation'].str.contains(\n",
    "    'event_name'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark changed data set\n",
    "TODO: repeat reports for the changed data set for domain and entities and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_reviewed_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=nlu_data_df)\n",
    "entity_reviewed_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure it is okay, you can save it as a csv file, make sure to name it correctly(i.e. `alarm_domain_first_review.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_scenario_df.to_csv('alarm_domain_first_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back up and check to make sure it looks okay. Make sure to give it the right name!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_domain_first_review_df = pd.read_csv(\n",
    "    'alarm_domain_first_review.csv', index_col=0)\n",
    "audio_domain_first_review_df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the evaluate_classifier in the NLU engine to check f1 score for intents and entities in the domain vs original NLU data of domain!\n",
    "# Value: benchmark!\n",
    "#TODO: implement a flow for getting the domains with the lowest f1 scores by intent/domain and entities and cleaning them by the order of the lowest f1 scores\n",
    "# TODO: concat all reviewed dfs and save to csv\n",
    "# TODO: add benchmark for whole NLU data set before and after cleaning! (by intents and domains!)\n",
    "# TODO: review the review marked entries\n",
    "# TODO: add new column for notes\n",
    "# TODO: change flow of review for only ones that should be reviewed, not all of the ones that have been changed (track changes by comparing against the original data set)\n",
    "# TODO: do the changed utterances have to be changed in other fields too or is it just enough for the tagged utterancve field?\n",
    "# TODO: add visualizations of domains, their intents, keywords in utterances, and entities to top\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

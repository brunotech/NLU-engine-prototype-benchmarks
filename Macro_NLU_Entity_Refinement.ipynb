{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nlu_engine import NLUEngine\n",
    "from nlu_engine import MacroDataRefinement\n",
    "from nlu_engine import MacroEntityRefinement\n",
    "\n",
    "from nlu_engine import DataUtils\n",
    "from nlu_engine import RenderJSON\n",
    "\n",
    "from nlu_engine import Analytics\n",
    "\n",
    "from nlu_engine import EntityExtractor, crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro NLU Data Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit like the TV show [Serverance](https://www.imdb.com/title/tt11280740/) .\n",
    "\n",
    "![Helly R and Mark S](https://media.npr.org/assets/img/2022/02/15/atv_severance_photo_010103-5f8033cc2b219ba64fe265ce893eae4c90e83896-s1100-c50.jpg \"Helly R and Mark G\")\n",
    "\n",
    "*Helly R*: `My job is to scroll through the spreadsheet and look for the numbers that feel scary?`\n",
    "\n",
    "*Mark S*: `I told you, youâ€™ll understand when you see it, so just be patient.`\n",
    "\n",
    "![MDR](https://www.imore.com/sites/imore.com/files/styles/large/public/field/image/2022/03/refinement-software-severance-apple-tv.jpg \"serverance micro data refinement\")\n",
    "\n",
    "*Helly R*: `That was scary. The numbers were scary.`\n",
    "\n",
    "Hopefully the intents and entities that are wrong aren't scary, just a bit frustrating. Let's see if we can find the right ones.\n",
    "\n",
    "NOTE: We will use Logistic Regression with TFIDF features to train our intent models and CRFs for entity exraction. Why? Well, they are very fast and both methods aren't state-of-the-art. This is good, because it is easier to find problems we will need to refine in the dataset than if we were to use a proper NLU engine like Snips or something SOTA like BERT. It is very important to note that some of the the problems we will pick up on, might not be an actual issue, but might be due to the limitations of the models. Refining the real problems and ignoring the limitations of the models is a good way to improve the models. Then when the dataset is ready, we can use some more advanced NLU engine and get the best performance possible.\n",
    "\n",
    "* Macro NLU Data Refinement: Intent\n",
    "* Macro NLU Data Refinement: Entity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded nlu_data_refined_df.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlu_data_df = pd.read_csv(\n",
    "        'data/refined/nlu_data_refined_df.csv', sep=',', index_col=0)\n",
    "    print('Successfully loaded nlu_data_refined_df.csv')\n",
    "except:\n",
    "    data = 'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    "    nlu_data_df = DataUtils.load_data(\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this when done.It's just for testing!\n",
    "data = 'data/NLU-Data-Home-Domain-Annotated-All-Cleaned.csv'\n",
    "nlu_data_df = DataUtils.load_data(\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `nlu_data_df['answer_normalised']` is taken from `nlu_data_df['answer_annotation']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df = DataUtils.convert_annotated_utterances_to_normalised_utterances(\n",
    "    nlu_data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove the unwanted entries for the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df = nlu_data_df[nlu_data_df['remove'] != True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity extraction report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entity extraction could be greatly improved by improving the features it uses. It would be great if someone would take a look at this. Perhaps the CRF features similar to what Snips uses would be better such as Brown clustering (probably)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement brown clustering to improve entity extraction (see entity_extractor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have the NLTK tokenizer to be able to extract entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this error featured in [this git issue](https://github.com/TeamHG-Memex/sklearn-crfsuite/issues/60) we have to use an older version of scikit learn (sklearn<0.24), otherwise the latest version would work. Hopefully this gets fixed one day.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=removed_nlu_data_refined_df, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartmoss/code/NLU-engine-prototype-benchmarks/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "removed_nlu_data_refined_df['entity_types'] = removed_nlu_data_refined_df['answer_annotation'].apply(\n",
    "    EntityExtractor.extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>answerid</th>\n",
       "      <th>notes</th>\n",
       "      <th>question</th>\n",
       "      <th>suggested_entities</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_normalised</th>\n",
       "      <th>scenario</th>\n",
       "      <th>intent</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>intent_refined</th>\n",
       "      <th>entity_refined</th>\n",
       "      <th>remove</th>\n",
       "      <th>status</th>\n",
       "      <th>answer_annotation</th>\n",
       "      <th>entity_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>wake me up at 5am this week</td>\n",
       "      <td>wake me up at five am this week</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : five am] [date : this week]</td>\n",
       "      <td>[{'type': 'time', 'words': ['five', 'am']}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>wake me up at 9am on Friday</td>\n",
       "      <td>wake me up at nine am on friday</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wake me up at [time : nine am] on [date : friday]</td>\n",
       "      <td>[{'type': 'time', 'words': ['nine', 'am']}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>date, time</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>set an alarm for two hours from now</td>\n",
       "      <td>alarm</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>alarm_set</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set an alarm for [time : two hours from now]</td>\n",
       "      <td>[{'type': 'time', 'words': ['two', 'hours', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Write what you would tell your PDA in the foll...</td>\n",
       "      <td>player_setting</td>\n",
       "      <td>Olly quiet!</td>\n",
       "      <td>quiet</td>\n",
       "      <td>audio</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>volume_mute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quiet</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How would you ask your PDA to change the light...</td>\n",
       "      <td>color_type</td>\n",
       "      <td>make the lighting bit more warm here</td>\n",
       "      <td>make the lighting a bit more warm here</td>\n",
       "      <td>iot</td>\n",
       "      <td>hue_lightchange</td>\n",
       "      <td>hue_lightup</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make the lighting a bit more [color_type : war...</td>\n",
       "      <td>[{'type': 'color_type', 'words': ['warm']}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25711</th>\n",
       "      <td>NaN</td>\n",
       "      <td>781.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, musch appreciated, olly.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25712</th>\n",
       "      <td>NaN</td>\n",
       "      <td>782.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you are hero, appreciated.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25713</th>\n",
       "      <td>NaN</td>\n",
       "      <td>783.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thanks, that's nice.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25714</th>\n",
       "      <td>NaN</td>\n",
       "      <td>784.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that's cool, thank you so much.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25715</th>\n",
       "      <td>NaN</td>\n",
       "      <td>785.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>general</td>\n",
       "      <td>praise</td>\n",
       "      <td>praise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>appreciate your answers, olly.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22392 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userid  answerid notes  \\\n",
       "0         1.0       1.0   NaN   \n",
       "1         1.0       2.0   NaN   \n",
       "2         1.0       3.0   NaN   \n",
       "3         1.0      31.0   NaN   \n",
       "7         1.0      38.0   NaN   \n",
       "...       ...       ...   ...   \n",
       "25711     NaN     781.0   NaN   \n",
       "25712     NaN     782.0   NaN   \n",
       "25713     NaN     783.0   NaN   \n",
       "25714     NaN     784.0   NaN   \n",
       "25715     NaN     785.0   NaN   \n",
       "\n",
       "                                                question suggested_entities  \\\n",
       "0      Write what you would tell your PDA in the foll...         date, time   \n",
       "1      Write what you would tell your PDA in the foll...         date, time   \n",
       "2      Write what you would tell your PDA in the foll...         date, time   \n",
       "3      Write what you would tell your PDA in the foll...     player_setting   \n",
       "7      How would you ask your PDA to change the light...         color_type   \n",
       "...                                                  ...                ...   \n",
       "25711                                                NaN                NaN   \n",
       "25712                                                NaN                NaN   \n",
       "25713                                                NaN                NaN   \n",
       "25714                                                NaN                NaN   \n",
       "25715                                                NaN                NaN   \n",
       "\n",
       "                                      answer  \\\n",
       "0                wake me up at 5am this week   \n",
       "1                wake me up at 9am on Friday   \n",
       "2        set an alarm for two hours from now   \n",
       "3                                Olly quiet!   \n",
       "7       make the lighting bit more warm here   \n",
       "...                                      ...   \n",
       "25711  that's cool, musch appreciated, olly.   \n",
       "25712             you are hero, appreciated.   \n",
       "25713                   thanks, that's nice.   \n",
       "25714        that's cool, thank you so much.   \n",
       "25715         appreciate your answers, olly.   \n",
       "\n",
       "                            answer_normalised scenario           intent  \\\n",
       "0             wake me up at five am this week    alarm        alarm_set   \n",
       "1             wake me up at nine am on friday    alarm        alarm_set   \n",
       "2         set an alarm for two hours from now    alarm        alarm_set   \n",
       "3                                       quiet    audio      volume_mute   \n",
       "7      make the lighting a bit more warm here      iot  hue_lightchange   \n",
       "...                                       ...      ...              ...   \n",
       "25711   that's cool, musch appreciated, olly.  general           praise   \n",
       "25712              you are hero, appreciated.  general           praise   \n",
       "25713                    thanks, that's nice.  general           praise   \n",
       "25714         that's cool, thank you so much.  general           praise   \n",
       "25715          appreciate your answers, olly.  general           praise   \n",
       "\n",
       "      predicted_label intent_refined  entity_refined remove status  \\\n",
       "0           alarm_set            NaN             NaN    NaN    NaN   \n",
       "1           alarm_set            NaN             NaN    NaN    NaN   \n",
       "2           alarm_set            NaN             NaN    NaN    NaN   \n",
       "3         volume_mute            NaN             NaN    NaN    NaN   \n",
       "7         hue_lightup           True             NaN  False    NaN   \n",
       "...               ...            ...             ...    ...    ...   \n",
       "25711          praise            NaN             NaN    NaN    NaN   \n",
       "25712          praise            NaN             NaN    NaN    NaN   \n",
       "25713          praise            NaN             NaN    NaN    NaN   \n",
       "25714          praise            NaN             NaN    NaN    NaN   \n",
       "25715          praise            NaN             NaN    NaN    NaN   \n",
       "\n",
       "                                       answer_annotation  \\\n",
       "0      wake me up at [time : five am] [date : this week]   \n",
       "1      wake me up at [time : nine am] on [date : friday]   \n",
       "2           set an alarm for [time : two hours from now]   \n",
       "3                                                  quiet   \n",
       "7      make the lighting a bit more [color_type : war...   \n",
       "...                                                  ...   \n",
       "25711              that's cool, musch appreciated, olly.   \n",
       "25712                         you are hero, appreciated.   \n",
       "25713                               thanks, that's nice.   \n",
       "25714                    that's cool, thank you so much.   \n",
       "25715                     appreciate your answers, olly.   \n",
       "\n",
       "                                            entity_types  \n",
       "0      [{'type': 'time', 'words': ['five', 'am']}, {'...  \n",
       "1      [{'type': 'time', 'words': ['nine', 'am']}, {'...  \n",
       "2      [{'type': 'time', 'words': ['two', 'hours', 'f...  \n",
       "3                                                     []  \n",
       "7            [{'type': 'color_type', 'words': ['warm']}]  \n",
       "...                                                  ...  \n",
       "25711                                                 []  \n",
       "25712                                                 []  \n",
       "25713                                                 []  \n",
       "25714                                                 []  \n",
       "25715                                                 []  \n",
       "\n",
       "[22392 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removed_nlu_data_refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: instead of getting domains for each entity type,\n",
    "# get entries for each domain and create classification report for each domain\n",
    "domain = 'general'\n",
    "domain_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['scenario'] == domain]\n",
    "#domain_entity_report_df = NLUEngine.evaluate_entity_classifier(data_df=domain_df, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df['entity_types'] = domain_df['answer_annotation'].apply(EntityExtractor.extract_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df['entity_types'] = domain_df['answer_annotation'].apply(\n",
    "    lambda utterance: re.findall(r'\\[(.*?)\\]', utterance))\n",
    "#TODO: have to find a way to get the entity type from the utterance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: get all domains, get overall scores and combine them into a single report\n",
    "domain_entity_report_df\n",
    "#TODO: graph this report\n",
    "#TODO: user picks a domain, then they see this report of the entity breakdown.\n",
    "# TODO: the user gets a data sheet for the domain arranged by support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor method to include domain as a possible input\n",
    "Analytics.plot_report(domain_entity_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = removed_nlu_data_refined_df['scenario'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_reports_for_domains(data_df):\n",
    "    domains = data_df['scenario'].unique().tolist()\n",
    "    domain_entity_reports_df = pd.DataFrame()\n",
    "    for domain in domains:\n",
    "        print(f'Evaluating entity classifier for {domain}')\n",
    "        domain_df = data_df[data_df['scenario'] == domain]\n",
    "        try:\n",
    "            domain_entity_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "                data_df=domain_df, cv=3)\n",
    "            domain_scores_df = domain_entity_report_df.tail(3)\n",
    "            domain_scores_df['domain'] = domain\n",
    "\n",
    "            domain_entity_reports_df.append(\n",
    "                domain_scores_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error evaluating entity classifier for {domain}')\n",
    "            print(e)\n",
    "\n",
    "    return domain_entity_reports_df\n",
    "\n",
    "domain_entity_reports_df = get_entity_reports_for_domains(domain_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = EntityExtractor.get_targets_and_labels(domain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EntityExtractor.train_crf_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = EntityExtractor.predict_crf_model(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Analytics.cross_validate_classifier(crf, X, y, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=domain_df, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df = NLUEngine.get_entity_reports_for_domains(removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_entity_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Extract the entities into something countable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = NLUEngine.train_entity_classifier(removed_nlu_data_refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/analytics/entity_tagger.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataUtils.pickle_model(classifier=crf_model, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model = DataUtils.import_pickled_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_utterance = 'set an alarm for [time : two hours] [time : from now]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_neigboring_same_entity_types(utterance, crf_model):\n",
    "    \"\"\"\n",
    "    Combines neighboring entities of the same type and removes them in an utterance, until all of the same neigboring entities are combined\n",
    "    \"\"\"\n",
    "    tagged_utterance = EntityExtractor.tag_utterance(\n",
    "        utterance,\n",
    "        crf_model\n",
    "    )\n",
    "    split_tagged_utterance = tagged_utterance.split(' ')\n",
    "\n",
    "    def combine_and_remove_entities(split_tagged_utterance):\n",
    "        \"\"\"\n",
    "        Combines neighboring entities of the same type and marks the duplicates to be removed.\n",
    "        NOTE: It is important for parsing to keep the same length, therefore we mark them instead of directly remove the matches.\n",
    "        \"\"\"\n",
    "        change_counter = 0\n",
    "        for index, token in enumerate(split_tagged_utterance):\n",
    "            if '[' in token:\n",
    "                if len(split_tagged_utterance) > index + 3:\n",
    "                    if token == split_tagged_utterance[index + 3]:\n",
    "                        split_tagged_utterance[index + 2] = split_tagged_utterance[index + 2].replace(\n",
    "                            ']', '') + ' ' + split_tagged_utterance[index + 5]\n",
    "\n",
    "                        split_tagged_utterance[index + 3] = 'to_remove'\n",
    "                        split_tagged_utterance[index + 4] = 'to_remove'\n",
    "                        split_tagged_utterance[index + 5] = 'to_remove'\n",
    "                        change_counter += 1\n",
    "        return (split_tagged_utterance, change_counter)\n",
    "\n",
    "    def remove_entities(split_tagged_utterance):\n",
    "        try:\n",
    "            while True:\n",
    "                split_tagged_utterance.remove('to_remove')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return split_tagged_utterance\n",
    "\n",
    "    combined_entities_split_tagged_utterance, change_counter = combine_and_remove_entities(\n",
    "        split_tagged_utterance)\n",
    "    removed_entities_split_tagged_utterance = remove_entities(\n",
    "        combined_entities_split_tagged_utterance)\n",
    "\n",
    "    while change_counter > 0:\n",
    "        combined_entities_split_tagged_utterance, change_counter = combine_and_remove_entities(\n",
    "            removed_entities_split_tagged_utterance)\n",
    "        removed_entities_split_tagged_utterance = remove_entities(\n",
    "            combined_entities_split_tagged_utterance)\n",
    "\n",
    "    return ' '.join(removed_entities_split_tagged_utterance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_neigboring_same_entity_types(utterance, crf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df['predicted_tagging'] = removed_nlu_data_refined_df['answer_normalised'].apply(\n",
    "    lambda x: combine_neigboring_same_entity_types(x, crf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation']\n",
    "                            != removed_nlu_data_refined_df['predicted_tagging']]\n",
    "# TODO: have user choose a domain, make report about entities in the domain,...\n",
    "# TODO: after the report, does the user clean all entities in the domain at once, or by individual entity type?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df.loc[21, 'answer_annotation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: It is probably a good idea to drop all of the ones that lack a good support.\n",
    "#NOTE: But it didn't work to fix the problem.\n",
    "remove_entities = [\n",
    "    'music_album',\n",
    "    'game_type',\n",
    "    \n",
    "]\n",
    "removed_nlu_data_refined__entities_cleaned_df = removed_nlu_data_refined_df[~removed_nlu_data_refined_df['answer_annotation'].str.contains('|'.join(remove_entities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Analytics.plot_report(entity_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove/replace worst: add in state features like here: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-check-what-classifier-learned\n",
    "# Specifically, we want print_state_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlu_engine import crf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model.state_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_state_features(Counter(crf.state_features_).most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: review the most common features, none of them are the word parts (chunks) or POS tags, are these even needed or helpful?\n",
    "# Can we remove them and speed up the process?\n",
    "# What other features could be used? Word2vec? Brown clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_selection = MacroDataRefinement.list_and_select_domain(nlu_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen from the entity extraction report, the entity extraction is not working for the alarm_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: review all scoring 0, see if they can be completely dropped or what\n",
    "entity_to_refine = 'alarm_type'\n",
    "nlu_scenario_df = removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation'].str.contains(\n",
    "    entity_to_refine)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_scenario_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entity(df, entity_to_remove):\n",
    "    \"\"\"\n",
    "        Remove all entries of an entity type from the dataframe.\n",
    "        :param df: pandas dataframe\n",
    "        :return: pandas dataframe\n",
    "        \"\"\"\n",
    "    updated_df = df.copy()\n",
    "    updated_df.loc[updated_df['answer_annotation'].str.contains(\n",
    "        entity_to_remove), 'remove'] = True\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = remove_entity(removed_nlu_data_refined_df, entity_to_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_nlu_data_refined_df[removed_nlu_data_refined_df['answer_annotation'].str.contains(\n",
    "    entity_to_refine)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Convert to ipysheet and review\n",
    "TODO: add in description of the types of fixes we can do to the NLU data for entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: same as above for intents but with predicted entities: report on them, break them down into a dictionary of dataframes and refine them.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example with 'alarm' and the alarm_type:\n",
    "* We see that the alarm_type entities are really event_name(ie wake up, soccer practice) except for ID 5879, we will need to change them to event_name and remove ID 5879.\n",
    "* The last one(ID 6320) is a mistake. Someone got confused with the prompt and assumed alarm is a security system. This is out of scope for the alarm domain, as the alarms are ones set on a phone or other device. We will drop this utterance.\n",
    "Once you are done reviewing, you convert it back to a dataframe and check to make sure it looks okay.\n",
    "Let's change all alarm_type entities to event_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewed_scenario_df['answer_annotation'] = reviewed_scenario_df['answer_annotation'].str.replace(\n",
    "    'alarm_type', 'event_name')\n",
    "reviewed_scenario_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay dokey, now we can merge this with the original data set and see if it made a difference already(well of course it did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_data_df.drop(\n",
    "    reviewed_scenario_df[reviewed_scenario_df['remove'] == True].index, inplace=True)\n",
    "\n",
    "reviewed_scenario_df = reviewed_scenario_df[~reviewed_scenario_df['remove'] == True]\n",
    "\n",
    "nlu_data_df.loc[nlu_data_df.index.intersection(\n",
    "    reviewed_scenario_df.index), 'answer_annotation'] = reviewed_scenario_df['answer_annotation']\n",
    "\n",
    "nlu_data_df[(nlu_data_df['scenario'].str.contains('alarm')) & (nlu_data_df['answer_annotation'].str.contains(\n",
    "    'event_name'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark changed data set\n",
    "TODO: repeat reports for the changed data set for domain and entities and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_reviewed_report_df = NLUEngine.evaluate_entity_classifier(\n",
    "    data_df=nlu_data_df)\n",
    "entity_reviewed_report_df.sort_values(by=['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are sure it is okay, you can save it as a csv file, make sure to name it correctly(i.e. `alarm_domain_first_review.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_scenario_df.to_csv('alarm_domain_first_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it back up and check to make sure it looks okay. Make sure to give it the right name!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_domain_first_review_df = pd.read_csv(\n",
    "    'alarm_domain_first_review.csv', index_col=0)\n",
    "audio_domain_first_review_df.tail(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the evaluate_classifier in the NLU engine to check f1 score for intents and entities in the domain vs original NLU data of domain!\n",
    "# Value: benchmark!\n",
    "#TODO: implement a flow for getting the domains with the lowest f1 scores by intent/domain and entities and cleaning them by the order of the lowest f1 scores\n",
    "# TODO: concat all reviewed dfs and save to csv\n",
    "# TODO: add benchmark for whole NLU data set before and after cleaning! (by intents and domains!)\n",
    "# TODO: review the review marked entries\n",
    "# TODO: add new column for notes\n",
    "# TODO: change flow of review for only ones that should be reviewed, not all of the ones that have been changed (track changes by comparing against the original data set)\n",
    "# TODO: do the changed utterances have to be changed in other fields too or is it just enough for the tagged utterancve field?\n",
    "# TODO: add visualizations of domains, their intents, keywords in utterances, and entities to top\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ecf1ecc6a840da86e8b827c66035ad900dc97d6a10e234826dd106c37257af"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
